{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import glob\n",
    "from dask import compute, delayed\n",
    "import dask.threaded\n",
    "import time\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:random_state not set so using default value\n",
      "WARNING:root:failed to load state from ./models/model-050.state: [Errno 2] No such file or directory: './models/model-050.state'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.064*\"deal\" + 0.047*\"iran\" + 0.024*\"israel\" + 0.022*\"russia\" + '\n",
      "  '0.017*\"agreement\" + 0.016*\"nuclear\" + 0.014*\"world\" + 0.013*\"war\" + '\n",
      "  '0.011*\"bomb\" + 0.011*\"russian\" + 0.011*\"support\" + 0.010*\"country\" + '\n",
      "  '0.010*\"jewish\" + 0.010*\"sanction\" + 0.009*\"putin\" + 0.009*\"peace\" + '\n",
      "  '0.008*\"iranian\" + 0.008*\"nation\" + 0.008*\"israeli\" + 0.008*\"cuba\" + '\n",
      "  '0.008*\"international\" + 0.007*\"jew\" + 0.007*\"ukraine\" + 0.007*\"west\" + '\n",
      "  '0.007*\"negotiate\" + 0.006*\"palestinian\" + 0.006*\"attack\" + 0.006*\"leader\" + '\n",
      "  '0.006*\"power\" + 0.006*\"japan\"'),\n",
      " (1,\n",
      "  '0.065*\"question\" + 0.061*\"point\" + 0.033*\"answer\" + 0.030*\"fact\" + '\n",
      "  '0.029*\"wrong\" + 0.028*\"agree\" + 0.027*\"argument\" + 0.023*\"reason\" + '\n",
      "  '0.018*\"case\" + 0.017*\"base\" + 0.015*\"correct\" + 0.015*\"simply\" + '\n",
      "  '0.013*\"make\" + 0.012*\"suggest\" + 0.011*\"view\" + 0.011*\"assume\" + '\n",
      "  '0.011*\"completely\" + 0.011*\"clear\" + 0.011*\"argue\" + 0.010*\"simple\" + '\n",
      "  '0.010*\"statement\" + 0.009*\"disagree\" + 0.009*\"response\" + 0.009*\"opinion\" + '\n",
      "  '0.008*\"reasonable\" + 0.008*\"logic\" + 0.008*\"position\" + 0.008*\"side\" + '\n",
      "  '0.008*\"present\" + 0.007*\"give\"'),\n",
      " (2,\n",
      "  '0.067*\"gun\" + 0.031*\"people\" + 0.028*\"control\" + 0.027*\"kill\" + '\n",
      "  '0.026*\"fear\" + 0.022*\"act\" + 0.021*\"violence\" + 0.018*\"shoot\" + '\n",
      "  '0.017*\"stop\" + 0.014*\"hand\" + 0.013*\"carry\" + 0.012*\"fire\" + 0.010*\"murder\" '\n",
      "  '+ 0.010*\"death\" + 0.009*\"threat\" + 0.009*\"weapon\" + 0.008*\"dangerous\" + '\n",
      "  '0.008*\"happen\" + 0.008*\"attack\" + 0.007*\"nra\" + 0.007*\"responsible\" + '\n",
      "  '0.007*\"public\" + 0.007*\"tragedy\" + 0.007*\"blame\" + 0.007*\"mass\" + '\n",
      "  '0.007*\"victim\" + 0.007*\"safety\" + 0.007*\"protect\" + 0.007*\"afraid\" + '\n",
      "  '0.007*\"safe\"'),\n",
      " (3,\n",
      "  '0.039*\"choice\" + 0.036*\"life\" + 0.032*\"abortion\" + 0.030*\"choose\" + '\n",
      "  '0.030*\"woman\" + 0.025*\"support\" + 0.024*\"sex\" + 0.023*\"make\" + '\n",
      "  '0.021*\"marriage\" + 0.017*\"decision\" + 0.017*\"gay\" + 0.013*\"force\" + '\n",
      "  '0.013*\"pro\" + 0.011*\"baby\" + 0.011*\"issue\" + 0.010*\"decide\" + '\n",
      "  '0.009*\"reason\" + 0.009*\"marry\" + 0.009*\"person\" + 0.008*\"give\" + '\n",
      "  '0.008*\"anti\" + 0.008*\"matter\" + 0.008*\"couple\" + 0.008*\"relationship\" + '\n",
      "  '0.008*\"personal\" + 0.007*\"legal\" + 0.007*\"birth_control\" + 0.006*\"birth\" + '\n",
      "  '0.006*\"planned_parenthood\" + 0.006*\"provide\"'),\n",
      " (4,\n",
      "  '0.057*\"fact\" + 0.040*\"lie\" + 0.036*\"claim\" + 0.027*\"truth\" + 0.020*\"true\" + '\n",
      "  '0.016*\"reality\" + 0.014*\"ignore\" + 0.014*\"real\" + 0.013*\"simply\" + '\n",
      "  '0.013*\"admit\" + 0.012*\"shame\" + 0.012*\"matter\" + 0.012*\"wrong\" + '\n",
      "  '0.011*\"prove\" + 0.011*\"public\" + 0.010*\"trust\" + 0.010*\"false\" + '\n",
      "  '0.009*\"face\" + 0.009*\"statement\" + 0.009*\"honest\" + 0.009*\"attempt\" + '\n",
      "  '0.008*\"hold\" + 0.008*\"call\" + 0.007*\"refuse\" + 0.007*\"excuse\" + '\n",
      "  '0.007*\"pretend\" + 0.007*\"hide\" + 0.006*\"accept\" + 0.006*\"action\" + '\n",
      "  '0.006*\"doubt\"'),\n",
      " (5,\n",
      "  '0.023*\"religious\" + 0.023*\"religion\" + 0.018*\"freedom\" + 0.017*\"christian\" '\n",
      "  '+ 0.016*\"church\" + 0.016*\"god\" + 0.015*\"belief\" + 0.014*\"society\" + '\n",
      "  '0.013*\"moral\" + 0.013*\"world\" + 0.011*\"culture\" + 0.010*\"faith\" + '\n",
      "  '0.010*\"practice\" + 0.008*\"muslim\" + 0.008*\"evil\" + 0.008*\"pope\" + '\n",
      "  '0.007*\"modern\" + 0.007*\"catholic\" + 0.007*\"respect\" + 0.006*\"base\" + '\n",
      "  '0.005*\"group\" + 0.005*\"accept\" + 0.005*\"true\" + 0.004*\"principle\" + '\n",
      "  '0.004*\"philosophy\" + 0.004*\"islam\" + 0.004*\"sin\" + 0.004*\"follow\" + '\n",
      "  '0.004*\"concept\" + 0.004*\"form\"'),\n",
      " (6,\n",
      "  '0.088*\"br\" + 0.058*\"amp\" + 0.056*\"http_www\" + 0.053*\"target\" + '\n",
      "  '0.050*\"href_http\" + 0.047*\"title\" + 0.036*\"article\" + 0.028*\"news\" + '\n",
      "  '0.025*\"www\" + 0.024*\"http\" + 0.018*\"org\" + 0.014*\"opinion\" + 0.010*\"blog\" + '\n",
      "  '0.010*\"html_target\" + 0.010*\"html_title\" + 0.010*\"report\" + 0.008*\"link\" + '\n",
      "  '0.008*\"gov\" + 0.007*\"www_nytime\" + 0.007*\"find\" + 0.006*\"story\" + '\n",
      "  '0.006*\"post\" + 0.005*\"html\" + 0.004*\"accord\" + 0.004*\"page\" + 0.004*\"read\" '\n",
      "  '+ 0.004*\"interesting\" + 0.004*\"net\" + 0.004*\"content\" + 0.004*\"show\"'),\n",
      " (7,\n",
      "  '0.075*\"school\" + 0.043*\"student\" + 0.039*\"college\" + 0.038*\"education\" + '\n",
      "  '0.027*\"high\" + 0.024*\"teacher\" + 0.022*\"public\" + 0.021*\"learn\" + '\n",
      "  '0.021*\"teach\" + 0.020*\"university\" + 0.015*\"class\" + 0.014*\"test\" + '\n",
      "  '0.013*\"degree\" + 0.011*\"year\" + 0.011*\"program\" + 0.011*\"kid\" + '\n",
      "  '0.010*\"graduate\" + 0.010*\"professor\" + 0.009*\"career\" + 0.008*\"math\" + '\n",
      "  '0.008*\"skill\" + 0.007*\"attend\" + 0.007*\"educate\" + 0.007*\"work\" + '\n",
      "  '0.006*\"major\" + 0.006*\"academic\" + 0.006*\"board\" + 0.006*\"job\" + '\n",
      "  '0.006*\"private\" + 0.006*\"grade\"'),\n",
      " (8,\n",
      "  '0.095*\"pay\" + 0.072*\"tax\" + 0.068*\"government\" + 0.029*\"income\" + '\n",
      "  '0.026*\"benefit\" + 0.025*\"public\" + 0.023*\"fund\" + 0.019*\"cost\" + '\n",
      "  '0.018*\"cut\" + 0.016*\"program\" + 0.016*\"service\" + 0.015*\"federal\" + '\n",
      "  '0.014*\"raise\" + 0.012*\"money\" + 0.010*\"taxpayer\" + 0.010*\"social_security\" '\n",
      "  '+ 0.010*\"provide\" + 0.010*\"private\" + 0.009*\"low\" + 0.009*\"receive\" + '\n",
      "  '0.008*\"budget\" + 0.008*\"retirement\" + 0.007*\"revenue\" + 0.007*\"dollar\" + '\n",
      "  '0.007*\"earn\" + 0.007*\"spending\" + 0.006*\"fee\" + 0.006*\"welfare\" + '\n",
      "  '0.006*\"spend\" + 0.006*\"support\"'),\n",
      " (9,\n",
      "  '0.025*\"drug\" + 0.015*\"exercise\" + 0.015*\"risk\" + 0.013*\"body\" + '\n",
      "  '0.011*\"healthy\" + 0.010*\"brain\" + 0.010*\"food\" + 0.010*\"weight\" + '\n",
      "  '0.010*\"disease\" + 0.009*\"pain\" + 0.009*\"eat\" + 0.009*\"effect\" + '\n",
      "  '0.009*\"study\" + 0.008*\"drink\" + 0.008*\"diet\" + 0.007*\"fat\" + '\n",
      "  '0.007*\"research\" + 0.006*\"vaccine\" + 0.006*\"calorie\" + 0.006*\"smoke\" + '\n",
      "  '0.005*\"problem\" + 0.005*\"sugar\" + 0.005*\"depression\" + 0.005*\"heart\" + '\n",
      "  '0.005*\"cancer\" + 0.005*\"normal\" + 0.005*\"cure\" + 0.005*\"alcohol\" + '\n",
      "  '0.005*\"condition\" + 0.005*\"physical\"'),\n",
      " (10,\n",
      "  '0.132*\"woman\" + 0.132*\"man\" + 0.022*\"male\" + 0.019*\"female\" + 0.019*\"young\" '\n",
      "  '+ 0.017*\"wear\" + 0.017*\"girl\" + 0.016*\"gender\" + 0.012*\"feel\" + 0.010*\"boy\" '\n",
      "  '+ 0.008*\"hair\" + 0.007*\"body\" + 0.007*\"sex\" + 0.006*\"dress\" + '\n",
      "  '0.006*\"sexual\" + 0.006*\"age\" + 0.006*\"face\" + 0.006*\"find\" + 0.006*\"ms\" + '\n",
      "  '0.006*\"fashion\" + 0.005*\"smile\" + 0.005*\"shoe\" + 0.005*\"clothe\" + '\n",
      "  '0.005*\"suit\" + 0.005*\"rape\" + 0.005*\"lady\" + 0.004*\"jenner\" + '\n",
      "  '0.004*\"transgender\" + 0.004*\"desire\" + 0.004*\"difference\"'),\n",
      " (11,\n",
      "  '0.053*\"high\" + 0.048*\"number\" + 0.037*\"increase\" + 0.036*\"rate\" + '\n",
      "  '0.033*\"low\" + 0.029*\"population\" + 0.026*\"large\" + 0.023*\"level\" + '\n",
      "  '0.017*\"average\" + 0.016*\"small\" + 0.015*\"result\" + 0.014*\"growth\" + '\n",
      "  '0.014*\"reduce\" + 0.014*\"compare\" + 0.013*\"percent\" + 0.011*\"rise\" + '\n",
      "  '0.010*\"standard\" + 0.009*\"due\" + 0.009*\"raise\" + 0.009*\"measure\" + '\n",
      "  '0.009*\"total\" + 0.007*\"percentage\" + 0.007*\"decline\" + 0.007*\"drop\" + '\n",
      "  '0.007*\"effect\" + 0.007*\"fall\" + 0.007*\"include\" + 0.007*\"factor\" + '\n",
      "  '0.006*\"inflation\" + 0.006*\"grow\"'),\n",
      " (12,\n",
      "  '0.061*\"thing\" + 0.044*\"talk\" + 0.038*\"guy\" + 0.035*\"happen\" + 0.026*\"stop\" '\n",
      "  '+ 0.019*\"bad\" + 0.018*\"real\" + 0.018*\"start\" + 0.017*\"hear\" + 0.017*\"head\" '\n",
      "  '+ 0.017*\"kind\" + 0.016*\"stupid\" + 0.016*\"folk\" + 0.016*\"put\" + 0.015*\"back\" '\n",
      "  '+ 0.015*\"call\" + 0.014*\"guess\" + 0.014*\"pretty\" + 0.013*\"expect\" + '\n",
      "  '0.011*\"turn\" + 0.011*\"smart\" + 0.011*\"good\" + 0.010*\"suppose\" + '\n",
      "  '0.010*\"stuff\" + 0.010*\"whatev\" + 0.010*\"lot\" + 0.009*\"sound\" + '\n",
      "  '0.009*\"throw\" + 0.009*\"sort\" + 0.008*\"worry\"'),\n",
      " (13,\n",
      "  '0.074*\"trump\" + 0.057*\"candidate\" + 0.029*\"republican\" + 0.028*\"debate\" + '\n",
      "  '0.023*\"run\" + 0.022*\"gop\" + 0.018*\"hillary\" + 0.018*\"campaign\" + '\n",
      "  '0.015*\"sander\" + 0.015*\"bernie_sander\" + 0.014*\"donald_trump\" + 0.014*\"win\" '\n",
      "  '+ 0.014*\"bernie\" + 0.013*\"presidential\" + 0.013*\"medium\" + 0.012*\"clinton\" '\n",
      "  '+ 0.009*\"fox\" + 0.008*\"primary\" + 0.008*\"supporter\" + 0.007*\"biden\" + '\n",
      "  '0.007*\"appeal\" + 0.007*\"party\" + 0.007*\"chance\" + 0.006*\"nomination\" + '\n",
      "  '0.006*\"hillary_clinton\" + 0.006*\"politic\" + 0.005*\"speak\" + '\n",
      "  '0.005*\"establishment\" + 0.005*\"poll\" + 0.005*\"election\"'),\n",
      " (14,\n",
      "  '0.232*\"good\" + 0.194*\"make\" + 0.077*\"bad\" + 0.060*\"thing\" + 0.042*\"idea\" + '\n",
      "  '0.025*\"sense\" + 0.018*\"point\" + 0.018*\"great\" + 0.014*\"mistake\" + '\n",
      "  '0.012*\"hard\" + 0.012*\"give\" + 0.011*\"easy\" + 0.011*\"reason\" + '\n",
      "  '0.010*\"difference\" + 0.010*\"hope\" + 0.009*\"chance\" + 0.009*\"decision\" + '\n",
      "  '0.009*\"lot\" + 0.008*\"agree\" + 0.008*\"luck\" + 0.007*\"perfect\" + '\n",
      "  '0.007*\"pretty\" + 0.007*\"put\" + 0.007*\"happen\" + 0.006*\"sound\" + '\n",
      "  '0.006*\"part\" + 0.005*\"difficult\" + 0.005*\"mind\" + 0.005*\"place\" + '\n",
      "  '0.005*\"impossible\"'),\n",
      " (15,\n",
      "  '0.037*\"market\" + 0.033*\"china\" + 0.028*\"price\" + 0.027*\"economy\" + '\n",
      "  '0.023*\"business\" + 0.022*\"trade\" + 0.018*\"free\" + 0.017*\"economic\" + '\n",
      "  '0.017*\"chinese\" + 0.016*\"profit\" + 0.016*\"sell\" + 0.015*\"buy\" + '\n",
      "  '0.014*\"corporation\" + 0.013*\"product\" + 0.011*\"industry\" + 0.011*\"interest\" '\n",
      "  '+ 0.011*\"corporate\" + 0.010*\"world\" + 0.010*\"stock\" + 0.010*\"big\" + '\n",
      "  '0.010*\"consumer\" + 0.009*\"create\" + 0.009*\"demand\" + 0.009*\"government\" + '\n",
      "  '0.009*\"investment\" + 0.009*\"feed\" + 0.008*\"financial\" + 0.008*\"global\" + '\n",
      "  '0.007*\"capital\" + 0.006*\"capitalism\"'),\n",
      " (16,\n",
      "  '0.084*\"read\" + 0.078*\"article\" + 0.054*\"time\" + 0.050*\"comment\" + '\n",
      "  '0.048*\"write\" + 0.037*\"story\" + 0.032*\"book\" + 0.023*\"nyt\" + 0.022*\"piece\" '\n",
      "  '+ 0.020*\"author\" + 0.017*\"reader\" + 0.014*\"report\" + 0.013*\"mention\" + '\n",
      "  '0.013*\"writer\" + 0.011*\"column\" + 0.010*\"paper\" + 0.010*\"new_york\" + '\n",
      "  '0.010*\"interesting\" + 0.010*\"news\" + 0.009*\"publish\" + 0.009*\"find\" + '\n",
      "  '0.009*\"post\" + 0.008*\"reading\" + 0.008*\"ms\" + 0.007*\"opinion\" + '\n",
      "  '0.007*\"newspaper\" + 0.006*\"reporter\" + 0.006*\"editorial\" + '\n",
      "  '0.006*\"journalist\" + 0.006*\"cover\"'),\n",
      " (17,\n",
      "  '0.051*\"love\" + 0.040*\"great\" + 0.020*\"hope\" + 0.017*\"feel\" + 0.015*\"life\" + '\n",
      "  '0.013*\"heart\" + 0.012*\"experience\" + 0.011*\"share\" + 0.011*\"bring\" + '\n",
      "  '0.011*\"find\" + 0.011*\"music\" + 0.011*\"hear\" + 0.011*\"wonderful\" + '\n",
      "  '0.010*\"enjoy\" + 0.010*\"mind\" + 0.010*\"listen\" + 0.009*\"eye\" + '\n",
      "  '0.009*\"moment\" + 0.009*\"art\" + 0.009*\"beautiful\" + 0.008*\"happy\" + '\n",
      "  '0.008*\"friend\" + 0.007*\"memory\" + 0.007*\"amazing\" + 0.006*\"learn\" + '\n",
      "  '0.006*\"sound\" + 0.006*\"remember\" + 0.006*\"song\" + 0.005*\"deep\" + '\n",
      "  '0.005*\"dream\"'),\n",
      " (18,\n",
      "  '0.089*\"care\" + 0.056*\"health\" + 0.033*\"cost\" + 0.030*\"insurance\" + '\n",
      "  '0.026*\"patient\" + 0.026*\"medical\" + 0.024*\"doctor\" + 0.022*\"plan\" + '\n",
      "  '0.018*\"system\" + 0.014*\"provide\" + 0.014*\"treatment\" + 0.013*\"healthcare\" + '\n",
      "  '0.012*\"hospital\" + 0.011*\"aca\" + 0.010*\"cover\" + 0.010*\"obamacare\" + '\n",
      "  '0.009*\"medicare\" + 0.008*\"service\" + 0.008*\"coverage\" + 0.008*\"physician\" + '\n",
      "  '0.007*\"good\" + 0.007*\"company\" + 0.007*\"therapist\" + 0.006*\"practice\" + '\n",
      "  '0.006*\"affordable\" + 0.006*\"treat\" + 0.006*\"premium\" + 0.006*\"therapy\" + '\n",
      "  '0.006*\"pay\" + 0.006*\"sick\"'),\n",
      " (19,\n",
      "  '0.081*\"issue\" + 0.031*\"understand\" + 0.026*\"important\" + 0.017*\"concern\" + '\n",
      "  '0.015*\"lack\" + 0.014*\"situation\" + 0.014*\"focus\" + 0.014*\"behavior\" + '\n",
      "  '0.012*\"address\" + 0.011*\"part\" + 0.011*\"discussion\" + 0.011*\"experience\" + '\n",
      "  '0.011*\"matter\" + 0.011*\"personal\" + 0.010*\"face\" + 0.009*\"regard\" + '\n",
      "  '0.009*\"difficult\" + 0.009*\"attention\" + 0.008*\"discuss\" + 0.008*\"involve\" + '\n",
      "  '0.008*\"approach\" + 0.008*\"response\" + 0.007*\"conversation\" + '\n",
      "  '0.007*\"reaction\" + 0.007*\"action\" + 0.006*\"perspective\" + 0.006*\"present\" + '\n",
      "  '0.006*\"problem\" + 0.006*\"agree\" + 0.006*\"engage\"'),\n",
      " (20,\n",
      "  '0.107*\"child\" + 0.076*\"family\" + 0.045*\"parent\" + 0.041*\"kid\" + '\n",
      "  '0.031*\"friend\" + 0.029*\"life\" + 0.028*\"young\" + 0.025*\"age\" + 0.023*\"live\" '\n",
      "  '+ 0.023*\"mother\" + 0.021*\"father\" + 0.020*\"home\" + 0.017*\"adult\" + '\n",
      "  '0.016*\"grow\" + 0.015*\"son\" + 0.014*\"bear\" + 0.013*\"daughter\" + 0.012*\"wife\" '\n",
      "  '+ 0.011*\"husband\" + 0.011*\"generation\" + 0.010*\"raise\" + 0.009*\"single\" + '\n",
      "  '0.007*\"brother\" + 0.007*\"mom\" + 0.007*\"love\" + 0.007*\"sister\" + 0.006*\"die\" '\n",
      "  '+ 0.006*\"feel\" + 0.006*\"dad\" + 0.006*\"learn\"'),\n",
      " (21,\n",
      "  '0.981*\"br\" + 0.000*\"dd\" + 0.000*\"colossal\" + 0.000*\"gt_gt\" + '\n",
      "  '0.000*\"albert_einstein\" + 0.000*\"kalidan\" + 0.000*\"prod\" + '\n",
      "  '0.000*\"unsurprisingly\" + 0.000*\"throwaway\" + 0.000*\"ter\" + 0.000*\"rot\" + '\n",
      "  '0.000*\"gt\" + 0.000*\"fell_apart\" + 0.000*\"michael_bain\" + 0.000*\"oar\" + '\n",
      "  '0.000*\"naught\" + 0.000*\"warp\" + 0.000*\"feeble\" + 0.000*\"abe_lincoln\" + '\n",
      "  '0.000*\"glorieta_new\" + 0.000*\"endlessly\" + 0.000*\"permanently\" + '\n",
      "  '0.000*\"lt_lt\" + 0.000*\"zip\" + 0.000*\"forewarn\" + 0.000*\"unmitigated\" + '\n",
      "  '0.000*\"crock\" + 0.000*\"marge\" + 0.000*\"undisputed\" + 0.000*\"modicum\"'),\n",
      " (22,\n",
      "  '0.110*\"problem\" + 0.059*\"poor\" + 0.033*\"system\" + 0.026*\"rich\" + '\n",
      "  '0.025*\"solution\" + 0.022*\"people\" + 0.021*\"economic\" + 0.021*\"society\" + '\n",
      "  '0.021*\"social\" + 0.020*\"create\" + 0.018*\"policy\" + 0.018*\"class\" + '\n",
      "  '0.017*\"real\" + 0.015*\"middle_class\" + 0.015*\"poverty\" + 0.015*\"work\" + '\n",
      "  '0.014*\"blame\" + 0.014*\"solve\" + 0.014*\"wealth\" + 0.013*\"fix\" + 0.012*\"lack\" '\n",
      "  '+ 0.012*\"opportunity\" + 0.011*\"wealthy\" + 0.009*\"fail\" + 0.008*\"address\" + '\n",
      "  '0.008*\"continue\" + 0.007*\"inequality\" + 0.007*\"result\" + 0.007*\"elite\" + '\n",
      "  '0.006*\"part\"'),\n",
      " (23,\n",
      "  '0.048*\"police\" + 0.036*\"crime\" + 0.024*\"criminal\" + 0.021*\"case\" + '\n",
      "  '0.020*\"officer\" + 0.016*\"cop\" + 0.016*\"prison\" + 0.015*\"justice\" + '\n",
      "  '0.014*\"charge\" + 0.012*\"victim\" + 0.011*\"commit\" + 0.011*\"murder\" + '\n",
      "  '0.010*\"jail\" + 0.010*\"arrest\" + 0.010*\"judge\" + 0.010*\"abuse\" + '\n",
      "  '0.009*\"system\" + 0.008*\"stop\" + 0.008*\"sentence\" + 0.008*\"innocent\" + '\n",
      "  '0.007*\"violent\" + 0.007*\"shoot\" + 0.007*\"guilty\" + 0.006*\"behavior\" + '\n",
      "  '0.006*\"trial\" + 0.006*\"situation\" + 0.006*\"kill\" + 0.006*\"person\" + '\n",
      "  '0.006*\"evidence\" + 0.005*\"suspect\"'),\n",
      " (24,\n",
      "  '0.038*\"water\" + 0.018*\"land\" + 0.017*\"grow\" + 0.016*\"energy\" + 0.013*\"oil\" '\n",
      "  '+ 0.013*\"california\" + 0.012*\"clean\" + 0.012*\"produce\" + 0.012*\"resource\" + '\n",
      "  '0.010*\"environment\" + 0.010*\"power\" + 0.009*\"plant\" + 0.009*\"air\" + '\n",
      "  '0.007*\"source\" + 0.007*\"waste\" + 0.006*\"build\" + 0.006*\"farm\" + '\n",
      "  '0.006*\"farmer\" + 0.006*\"wind\" + 0.006*\"industry\" + 0.006*\"large\" + '\n",
      "  '0.006*\"big\" + 0.006*\"gas\" + 0.006*\"coal\" + 0.005*\"environmental\" + '\n",
      "  '0.005*\"area\" + 0.005*\"supply\" + 0.005*\"fuel\" + 0.005*\"natural\" + '\n",
      "  '0.005*\"sea\"'),\n",
      " (25,\n",
      "  '0.030*\"make\" + 0.026*\"food\" + 0.024*\"add\" + 0.024*\"eat\" + 0.015*\"good\" + '\n",
      "  '0.010*\"serve\" + 0.010*\"recipe\" + 0.009*\"easy\" + 0.008*\"restaurant\" + '\n",
      "  '0.008*\"cook\" + 0.008*\"bit\" + 0.007*\"great\" + 0.007*\"taste\" + 0.006*\"table\" '\n",
      "  '+ 0.006*\"tip\" + 0.006*\"cut\" + 0.006*\"fresh\" + 0.006*\"nice\" + '\n",
      "  '0.006*\"chicken\" + 0.005*\"meat\" + 0.005*\"store\" + 0.005*\"meal\" + '\n",
      "  '0.005*\"minute\" + 0.005*\"half\" + 0.005*\"dinner\" + 0.005*\"wine\" + '\n",
      "  '0.005*\"delicious\" + 0.005*\"hot\" + 0.004*\"egg\" + 0.004*\"top\"'),\n",
      " (26,\n",
      "  '0.090*\"republican\" + 0.089*\"vote\" + 0.066*\"party\" + 0.038*\"democrat\" + '\n",
      "  '0.028*\"election\" + 0.025*\"voter\" + 0.025*\"democratic\" + 0.024*\"support\" + '\n",
      "  '0.020*\"political\" + 0.017*\"politician\" + 0.017*\"congress\" + '\n",
      "  '0.017*\"democracy\" + 0.015*\"gop\" + 0.015*\"majority\" + 0.013*\"voting\" + '\n",
      "  '0.010*\"house\" + 0.010*\"interest\" + 0.009*\"elect\" + 0.009*\"win\" + '\n",
      "  '0.008*\"policy\" + 0.008*\"represent\" + 0.007*\"senate\" + 0.007*\"member\" + '\n",
      "  '0.007*\"representative\" + 0.007*\"senator\" + 0.007*\"issue\" + 0.007*\"politic\" '\n",
      "  '+ 0.007*\"poll\" + 0.006*\"base\" + 0.006*\"favor\"'),\n",
      " (27,\n",
      "  '0.088*\"law\" + 0.080*\"state\" + 0.035*\"rule\" + 0.027*\"court\" + 0.020*\"case\" + '\n",
      "  '0.017*\"legal\" + 0.016*\"constitution\" + 0.015*\"government\" + '\n",
      "  '0.014*\"decision\" + 0.014*\"act\" + 0.013*\"require\" + 0.013*\"texas\" + '\n",
      "  '0.013*\"citizen\" + 0.012*\"pass\" + 0.011*\"federal\" + 0.011*\"justice\" + '\n",
      "  '0.010*\"lawyer\" + 0.009*\"protect\" + 0.009*\"judge\" + 0.009*\"congress\" + '\n",
      "  '0.009*\"apply\" + 0.007*\"robert\" + 0.007*\"decide\" + 0.006*\"intent\" + '\n",
      "  '0.006*\"process\" + 0.006*\"constitutional\" + 0.006*\"establish\" + 0.005*\"set\" '\n",
      "  '+ 0.005*\"enforce\" + 0.005*\"order\"'),\n",
      " (28,\n",
      "  '0.031*\"science\" + 0.018*\"study\" + 0.016*\"climate_change\" + 0.015*\"climate\" '\n",
      "  '+ 0.014*\"scientist\" + 0.013*\"datum\" + 0.013*\"change\" + 0.013*\"research\" + '\n",
      "  '0.012*\"show\" + 0.011*\"evidence\" + 0.011*\"base\" + 0.011*\"model\" + '\n",
      "  '0.009*\"rise\" + 0.009*\"earth\" + 0.008*\"scientific\" + 0.008*\"warm\" + '\n",
      "  '0.008*\"theory\" + 0.008*\"global\" + 0.007*\"temperature\" + 0.007*\"find\" + '\n",
      "  '0.007*\"record\" + 0.007*\"planet\" + 0.006*\"result\" + 0.006*\"effect\" + '\n",
      "  '0.006*\"expert\" + 0.006*\"paper\" + 0.005*\"trend\" + 0.005*\"year\" + '\n",
      "  '0.005*\"decade\" + 0.005*\"analysis\"'),\n",
      " (29,\n",
      "  '0.063*\"greece\" + 0.044*\"greek\" + 0.039*\"debt\" + 0.026*\"bank\" + '\n",
      "  '0.023*\"germany\" + 0.018*\"europe\" + 0.018*\"german\" + 0.018*\"economy\" + '\n",
      "  '0.018*\"country\" + 0.017*\"government\" + 0.016*\"loan\" + 0.015*\"european\" + '\n",
      "  '0.015*\"euro\" + 0.015*\"economic\" + 0.013*\"crisis\" + 0.011*\"financial\" + '\n",
      "  '0.011*\"money\" + 0.011*\"austerity\" + 0.009*\"pay\" + 0.008*\"krugman\" + '\n",
      "  '0.008*\"currency\" + 0.007*\"reform\" + 0.006*\"borrow\" + 0.006*\"banker\" + '\n",
      "  '0.006*\"default\" + 0.006*\"creditor\" + 0.006*\"leave\" + 0.006*\"union\" + '\n",
      "  '0.005*\"eurozone\" + 0.005*\"demand\"'),\n",
      " (30,\n",
      "  '0.495*\"br\" + 0.036*\"give\" + 0.032*\"call\" + 0.031*\"free\" + 0.017*\"speech\" + '\n",
      "  '0.015*\"part\" + 0.014*\"follow\" + 0.012*\"include\" + 0.012*\"list\" + '\n",
      "  '0.011*\"today\" + 0.011*\"watch\" + 0.008*\"offer\" + 0.008*\"line\" + 0.007*\"hold\" '\n",
      "  '+ 0.007*\"leave\" + 0.007*\"true\" + 0.007*\"org_wiki\" + 0.007*\"en_wikipedia\" + '\n",
      "  '0.007*\"target_https\" + 0.007*\"title_https\" + 0.006*\"href_https\" + '\n",
      "  '0.006*\"entire\" + 0.006*\"www_youtube\" + 0.006*\"real\" + 0.005*\"note\" + '\n",
      "  '0.005*\"full\" + 0.005*\"dear\" + 0.005*\"break\" + 0.005*\"rest\" + '\n",
      "  '0.005*\"mention\"'),\n",
      " (31,\n",
      "  '0.078*\"show\" + 0.035*\"watch\" + 0.017*\"tv\" + 0.016*\"movie\" + '\n",
      "  '0.015*\"character\" + 0.010*\"funny\" + 0.009*\"video\" + 0.009*\"film\" + '\n",
      "  '0.009*\"picture\" + 0.008*\"miss\" + 0.007*\"story\" + 0.007*\"photo\" + '\n",
      "  '0.007*\"real\" + 0.007*\"laugh\" + 0.007*\"audience\" + 0.006*\"interview\" + '\n",
      "  '0.006*\"interesting\" + 0.006*\"scene\" + 0.006*\"joke\" + 0.006*\"episode\" + '\n",
      "  '0.005*\"season\" + 0.005*\"series\" + 0.005*\"review\" + 0.005*\"line\" + '\n",
      "  '0.005*\"star\" + 0.005*\"actor\" + 0.005*\"television\" + 0.005*\"news\" + '\n",
      "  '0.005*\"king\" + 0.005*\"reality\"'),\n",
      " (32,\n",
      "  '0.032*\"drive\" + 0.028*\"car\" + 0.014*\"line\" + 0.013*\"run\" + 0.013*\"travel\" + '\n",
      "  '0.012*\"walk\" + 0.012*\"road\" + 0.011*\"back\" + 0.010*\"stop\" + 0.010*\"mile\" + '\n",
      "  '0.009*\"train\" + 0.009*\"time\" + 0.008*\"foot\" + 0.008*\"front\" + 0.008*\"put\" + '\n",
      "  '0.008*\"horse\" + 0.008*\"fly\" + 0.008*\"driver\" + 0.008*\"park\" + 0.008*\"trip\" '\n",
      "  '+ 0.007*\"open\" + 0.007*\"check\" + 0.007*\"wait\" + 0.007*\"seat\" + '\n",
      "  '0.007*\"close\" + 0.007*\"ride\" + 0.007*\"hit\" + 0.007*\"jump\" + 0.007*\"place\" + '\n",
      "  '0.006*\"airline\"'),\n",
      " (33,\n",
      "  '0.096*\"president\" + 0.064*\"obama\" + 0.031*\"bush\" + 0.026*\"run\" + '\n",
      "  '0.019*\"bill\" + 0.018*\"office\" + 0.017*\"state\" + 0.016*\"elect\" + '\n",
      "  '0.016*\"administration\" + 0.015*\"policy\" + 0.015*\"governor\" + '\n",
      "  '0.014*\"clinton\" + 0.011*\"walker\" + 0.011*\"reagan\" + 0.009*\"mr\" + '\n",
      "  '0.009*\"great\" + 0.009*\"fail\" + 0.009*\"jeb\" + 0.008*\"house\" + '\n",
      "  '0.008*\"remember\" + 0.008*\"presidency\" + 0.007*\"legacy\" + 0.007*\"promise\" + '\n",
      "  '0.007*\"wisconsin\" + 0.007*\"christie\" + 0.007*\"rubio\" + 0.007*\"florida\" + '\n",
      "  '0.007*\"leader\" + 0.006*\"senator\" + 0.006*\"leadership\"'),\n",
      " (34,\n",
      "  '0.106*\"black\" + 0.093*\"white\" + 0.045*\"race\" + 0.030*\"american\" + '\n",
      "  '0.027*\"group\" + 0.023*\"people\" + 0.019*\"community\" + 0.018*\"african\" + '\n",
      "  '0.018*\"racism\" + 0.017*\"racist\" + 0.017*\"culture\" + 0.015*\"matter\" + '\n",
      "  '0.015*\"racial\" + 0.013*\"color\" + 0.012*\"identify\" + 0.010*\"person\" + '\n",
      "  '0.010*\"minority\" + 0.009*\"privilege\" + 0.008*\"america\" + 0.008*\"blow\" + '\n",
      "  '0.008*\"identity\" + 0.007*\"discrimination\" + 0.006*\"cultural\" + '\n",
      "  '0.006*\"society\" + 0.006*\"difference\" + 0.006*\"hispanic\" + '\n",
      "  '0.005*\"individual\" + 0.005*\"skin\" + 0.005*\"asian\" + 0.004*\"hate\"'),\n",
      " (35,\n",
      "  '0.051*\"change\" + 0.034*\"power\" + 0.025*\"future\" + 0.016*\"lead\" + '\n",
      "  '0.016*\"step\" + 0.015*\"continue\" + 0.011*\"goal\" + 0.011*\"individual\" + '\n",
      "  '0.011*\"result\" + 0.011*\"current\" + 0.010*\"hope\" + 0.010*\"end\" + '\n",
      "  '0.010*\"achieve\" + 0.010*\"effort\" + 0.010*\"interest\" + 0.010*\"strong\" + '\n",
      "  '0.008*\"remain\" + 0.008*\"order\" + 0.008*\"seek\" + 0.008*\"process\" + '\n",
      "  '0.008*\"success\" + 0.008*\"progress\" + 0.008*\"political\" + 0.008*\"forward\" + '\n",
      "  '0.008*\"create\" + 0.007*\"control\" + 0.007*\"ability\" + 0.007*\"challenge\" + '\n",
      "  '0.007*\"path\" + 0.007*\"influence\"'),\n",
      " (36,\n",
      "  '0.021*\"information\" + 0.014*\"security\" + 0.014*\"internet\" + '\n",
      "  '0.012*\"technology\" + 0.011*\"service\" + 0.011*\"phone\" + 0.011*\"secret\" + '\n",
      "  '0.010*\"email\" + 0.010*\"computer\" + 0.010*\"government\" + 0.009*\"datum\" + '\n",
      "  '0.009*\"personal\" + 0.009*\"find\" + 0.009*\"call\" + 0.008*\"ad\" + '\n",
      "  '0.008*\"access\" + 0.008*\"send\" + 0.008*\"public\" + 0.008*\"account\" + '\n",
      "  '0.007*\"record\" + 0.007*\"check\" + 0.007*\"apple\" + 0.007*\"provide\" + '\n",
      "  '0.007*\"online\" + 0.006*\"search\" + 0.006*\"site\" + 0.006*\"privacy\" + '\n",
      "  '0.006*\"private\" + 0.006*\"agency\" + 0.006*\"include\"'),\n",
      " (37,\n",
      "  '0.109*\"money\" + 0.067*\"make\" + 0.053*\"give\" + 0.053*\"big\" + 0.034*\"spend\" + '\n",
      "  '0.027*\"buy\" + 0.020*\"dollar\" + 0.017*\"time\" + 0.016*\"million\" + 0.016*\"put\" '\n",
      "  '+ 0.015*\"save\" + 0.015*\"lose\" + 0.015*\"sell\" + 0.014*\"worth\" + 0.014*\"back\" '\n",
      "  '+ 0.012*\"hand\" + 0.011*\"lot\" + 0.010*\"billion\" + 0.010*\"huge\" + '\n",
      "  '0.009*\"thousand\" + 0.008*\"amount\" + 0.008*\"waste\" + 0.008*\"hundred\" + '\n",
      "  '0.007*\"run\" + 0.007*\"politician\" + 0.007*\"real\" + 0.007*\"cash\" + '\n",
      "  '0.006*\"good\" + 0.006*\"small\" + 0.006*\"manage\"'),\n",
      " (38,\n",
      "  '0.375*\"people\" + 0.042*\"person\" + 0.041*\"feel\" + 0.036*\"make\" + '\n",
      "  '0.033*\"thing\" + 0.033*\"live\" + 0.026*\"lot\" + 0.021*\"understand\" + '\n",
      "  '0.020*\"find\" + 0.014*\"kind\" + 0.014*\"hard\" + 0.012*\"hurt\" + 0.011*\"place\" + '\n",
      "  '0.010*\"reason\" + 0.010*\"wrong\" + 0.009*\"realize\" + 0.009*\"happen\" + '\n",
      "  '0.008*\"change\" + 0.008*\"talk\" + 0.008*\"life\" + 0.007*\"put\" + 0.006*\"sad\" + '\n",
      "  '0.006*\"happy\" + 0.006*\"deserve\" + 0.006*\"treat\" + 0.005*\"matter\" + '\n",
      "  '0.005*\"personally\" + 0.005*\"simply\" + 0.004*\"decent\" + 0.004*\"respect\"'),\n",
      " (39,\n",
      "  '0.068*\"war\" + 0.027*\"fight\" + 0.025*\"iraq\" + 0.025*\"isis\" + '\n",
      "  '0.023*\"military\" + 0.019*\"force\" + 0.010*\"attack\" + 0.010*\"group\" + '\n",
      "  '0.010*\"enemy\" + 0.010*\"support\" + 0.010*\"middle_east\" + 0.009*\"terrorist\" + '\n",
      "  '0.009*\"world\" + 0.009*\"send\" + 0.008*\"government\" + 0.007*\"army\" + '\n",
      "  '0.007*\"destroy\" + 0.007*\"serve\" + 0.007*\"country\" + 0.006*\"syria\" + '\n",
      "  '0.006*\"turkey\" + 0.006*\"soldier\" + 0.006*\"vietnam\" + 0.006*\"troop\" + '\n",
      "  '0.006*\"end\" + 0.006*\"conflict\" + 0.006*\"muslim\" + 0.006*\"battle\" + '\n",
      "  '0.006*\"threat\" + 0.005*\"defeat\"'),\n",
      " (40,\n",
      "  '0.100*\"day\" + 0.097*\"work\" + 0.088*\"time\" + 0.034*\"week\" + 0.029*\"hour\" + '\n",
      "  '0.021*\"hard\" + 0.018*\"leave\" + 0.016*\"back\" + 0.014*\"sit\" + 0.013*\"night\" + '\n",
      "  '0.013*\"wait\" + 0.011*\"put\" + 0.011*\"summer\" + 0.011*\"long\" + 0.011*\"stay\" + '\n",
      "  '0.010*\"office\" + 0.010*\"month\" + 0.010*\"room\" + 0.010*\"find\" + 0.010*\"end\" '\n",
      "  '+ 0.010*\"start\" + 0.009*\"sleep\" + 0.008*\"set\" + 0.008*\"full\" + 0.008*\"late\" '\n",
      "  '+ 0.008*\"break\" + 0.007*\"spend\" + 0.007*\"early\" + 0.007*\"turn\" + '\n",
      "  '0.007*\"walk\"'),\n",
      " (41,\n",
      "  '0.056*\"word\" + 0.025*\"speak\" + 0.020*\"hear\" + 0.019*\"comment\" + '\n",
      "  '0.016*\"find\" + 0.013*\"language\" + 0.013*\"refer\" + 0.013*\"mind\" + '\n",
      "  '0.013*\"guess\" + 0.012*\"sound\" + 0.012*\"describe\" + 0.012*\"call\" + '\n",
      "  '0.011*\"post\" + 0.010*\"mention\" + 0.009*\"puzzle\" + 0.009*\"explain\" + '\n",
      "  '0.008*\"fill\" + 0.008*\"clue\" + 0.007*\"point\" + 0.007*\"bit\" + 0.007*\"fit\" + '\n",
      "  '0.007*\"term\" + 0.007*\"note\" + 0.007*\"thought\" + 0.007*\"letter\" + '\n",
      "  '0.007*\"today\" + 0.007*\"kind\" + 0.007*\"phrase\" + 0.007*\"understand\" + '\n",
      "  '0.006*\"agree\"'),\n",
      " (42,\n",
      "  '0.051*\"state\" + 0.043*\"history\" + 0.028*\"flag\" + 0.024*\"south\" + '\n",
      "  '0.019*\"slavery\" + 0.017*\"war\" + 0.014*\"civil\" + 0.014*\"today\" + '\n",
      "  '0.013*\"southern\" + 0.012*\"confederate_flag\" + 0.012*\"slave\" + '\n",
      "  '0.012*\"symbol\" + 0.011*\"fight\" + 0.011*\"represent\" + 0.011*\"fly\" + '\n",
      "  '0.011*\"remove\" + 0.010*\"american\" + 0.009*\"honor\" + 0.008*\"stand\" + '\n",
      "  '0.008*\"part\" + 0.007*\"nation\" + 0.007*\"hate\" + 0.007*\"display\" + '\n",
      "  '0.007*\"union\" + 0.007*\"north\" + 0.006*\"united_state\" + 0.006*\"red\" + '\n",
      "  '0.006*\"great\" + 0.006*\"racist\" + 0.006*\"proud\"'),\n",
      " (43,\n",
      "  '0.047*\"play\" + 0.037*\"game\" + 0.026*\"team\" + 0.022*\"sport\" + 0.018*\"player\" '\n",
      "  '+ 0.017*\"great\" + 0.015*\"world\" + 0.014*\"good\" + 0.014*\"win\" + 0.012*\"fan\" '\n",
      "  '+ 0.009*\"football\" + 0.009*\"field\" + 0.008*\"big\" + 0.008*\"soccer\" + '\n",
      "  '0.008*\"match\" + 0.007*\"ball\" + 0.007*\"hit\" + 0.007*\"head\" + 0.006*\"final\" + '\n",
      "  '0.006*\"athlete\" + 0.006*\"watch\" + 0.006*\"top\" + 0.006*\"fifa\" + 0.006*\"goal\" '\n",
      "  '+ 0.005*\"club\" + 0.005*\"run\" + 0.005*\"beat\" + 0.005*\"cup\" + 0.005*\"time\" + '\n",
      "  '0.005*\"professional\"'),\n",
      " (44,\n",
      "  '0.221*\"year\" + 0.102*\"time\" + 0.084*\"long\" + 0.043*\"ago\" + 0.029*\"start\" + '\n",
      "  '0.023*\"term\" + 0.022*\"back\" + 0.020*\"end\" + 0.018*\"begin\" + 0.017*\"happen\" '\n",
      "  '+ 0.017*\"month\" + 0.017*\"change\" + 0.016*\"today\" + 0.016*\"past\" + '\n",
      "  '0.015*\"lose\" + 0.014*\"decade\" + 0.012*\"early\" + 0.011*\"finally\" + '\n",
      "  '0.010*\"continue\" + 0.010*\"longer\" + 0.010*\"move\" + 0.010*\"period\" + '\n",
      "  '0.009*\"short\" + 0.008*\"late\" + 0.007*\"remember\" + 0.007*\"age\" + '\n",
      "  '0.006*\"realize\" + 0.006*\"leave\" + 0.006*\"couple\" + 0.005*\"wait\"'),\n",
      " (45,\n",
      "  '0.044*\"conservative\" + 0.038*\"liberal\" + 0.028*\"medium\" + 0.028*\"political\" '\n",
      "  '+ 0.025*\"leave\" + 0.020*\"side\" + 0.019*\"wing\" + 0.019*\"view\" + 0.013*\"call\" '\n",
      "  '+ 0.012*\"anti\" + 0.012*\"hate\" + 0.012*\"social\" + 0.011*\"politic\" + '\n",
      "  '0.011*\"mr\" + 0.009*\"progressive\" + 0.008*\"brook\" + 0.008*\"idea\" + '\n",
      "  '0.008*\"extreme\" + 0.007*\"point\" + 0.007*\"ideology\" + 0.007*\"agenda\" + '\n",
      "  '0.007*\"left\" + 0.006*\"stewart\" + 0.006*\"center\" + 0.005*\"politician\" + '\n",
      "  '0.005*\"radical\" + 0.005*\"propaganda\" + 0.005*\"public\" + 0.005*\"daily\" + '\n",
      "  '0.005*\"voice\"'),\n",
      " (46,\n",
      "  '0.056*\"human\" + 0.054*\"life\" + 0.032*\"live\" + 0.028*\"world\" + 0.024*\"die\" + '\n",
      "  '0.024*\"death\" + 0.023*\"kill\" + 0.022*\"animal\" + 0.014*\"dog\" + '\n",
      "  '0.013*\"suffer\" + 0.010*\"nature\" + 0.010*\"lion\" + 0.009*\"end\" + 0.008*\"save\" '\n",
      "  '+ 0.008*\"survive\" + 0.008*\"people\" + 0.007*\"humanity\" + 0.007*\"planet\" + '\n",
      "  '0.007*\"hunt\" + 0.006*\"destroy\" + 0.006*\"specie\" + 0.006*\"loss\" + '\n",
      "  '0.006*\"dead\" + 0.006*\"cat\" + 0.006*\"understand\" + 0.005*\"alive\" + '\n",
      "  '0.005*\"protect\" + 0.005*\"natural\" + 0.005*\"human_being\" + 0.005*\"cecil\"'),\n",
      " (47,\n",
      "  '0.090*\"job\" + 0.081*\"work\" + 0.046*\"worker\" + 0.046*\"company\" + 0.034*\"pay\" '\n",
      "  '+ 0.032*\"business\" + 0.028*\"employee\" + 0.022*\"wage\" + 0.017*\"union\" + '\n",
      "  '0.015*\"amazon\" + 0.015*\"hire\" + 0.013*\"labor\" + 0.010*\"corporate\" + '\n",
      "  '0.010*\"employer\" + 0.009*\"ceo\" + 0.009*\"disney\" + 0.009*\"corporation\" + '\n",
      "  '0.008*\"management\" + 0.008*\"replace\" + 0.008*\"benefit\" + 0.007*\"salary\" + '\n",
      "  '0.006*\"tech\" + 0.006*\"customer\" + 0.006*\"profit\" + 0.006*\"low\" + '\n",
      "  '0.006*\"skill\" + 0.006*\"visa\" + 0.006*\"minimum_wage\" + 0.006*\"executive\" + '\n",
      "  '0.005*\"employ\"'),\n",
      " (48,\n",
      "  '0.139*\"american\" + 0.133*\"country\" + 0.058*\"america\" + 0.044*\"world\" + '\n",
      "  '0.032*\"nation\" + 0.032*\"citizen\" + 0.018*\"usa\" + 0.015*\"immigrant\" + '\n",
      "  '0.013*\"europe\" + 0.013*\"united_state\" + 0.011*\"illegal\" + '\n",
      "  '0.011*\"immigration\" + 0.011*\"border\" + 0.010*\"mexico\" + 0.009*\"foreign\" + '\n",
      "  '0.008*\"european\" + 0.008*\"india\" + 0.008*\"canada\" + 0.007*\"national\" + '\n",
      "  '0.007*\"bear\" + 0.007*\"refugee\" + 0.007*\"africa\" + 0.006*\"mexican\" + '\n",
      "  '0.006*\"rest\" + 0.006*\"french\" + 0.006*\"bring\" + 0.006*\"france\" + '\n",
      "  '0.006*\"million\" + 0.005*\"indian\" + 0.005*\"native\"'),\n",
      " (49,\n",
      "  '0.047*\"live\" + 0.042*\"city\" + 0.027*\"place\" + 0.025*\"move\" + 0.025*\"home\" + '\n",
      "  '0.023*\"area\" + 0.021*\"house\" + 0.019*\"build\" + 0.017*\"new_york\" + '\n",
      "  '0.016*\"local\" + 0.016*\"neighborhood\" + 0.014*\"building\" + 0.013*\"housing\" + '\n",
      "  '0.013*\"nyc\" + 0.012*\"town\" + 0.011*\"street\" + 0.010*\"community\" + '\n",
      "  '0.010*\"visit\" + 0.009*\"rent\" + 0.009*\"small\" + 0.008*\"resident\" + '\n",
      "  '0.007*\"neighbor\" + 0.006*\"mayor\" + 0.006*\"property\" + 0.006*\"project\" + '\n",
      "  '0.006*\"nice\" + 0.005*\"leave\" + 0.005*\"apartment\" + 0.005*\"large\" + '\n",
      "  '0.005*\"afford\"')]\n"
     ]
    }
   ],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet' # update this path\n",
    "\n",
    "mallet_model=gensim.models.LdaModel.load('./models/model-050')\n",
    "\n",
    "pprint(mallet_model.print_topics(50,30))\n",
    "\n",
    "##unwrap the model\n",
    "model=gensim.models.wrappers.ldamallet.malletmodel2ldamodel(mallet_model, gamma_threshold=0.001, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0\n",
      "deal  iran  israel  russia  agreement  nuclear  world  war  bomb  russian  support  country  jewish  sanction  putin  peace  iranian  nation  israeli  cuba  international  jew  ukraine  west  negotiate  palestinian  attack  leader  power  japan\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic1\n",
      "question  point  answer  fact  wrong  agree  argument  reason  case  base  correct  simply  make  suggest  view  assume  completely  clear  argue  simple  statement  disagree  response  opinion  reasonable  logic  position  side  present  give\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic2\n",
      "gun  people  control  kill  fear  act  violence  shoot  stop  hand  carry  fire  murder  death  threat  weapon  dangerous  happen  attack  nra  responsible  public  tragedy  blame  mass  victim  safety  protect  afraid  safe\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic3\n",
      "choice  life  abortion  choose  woman  support  sex  make  marriage  decision  gay  force  pro  baby  issue  decide  reason  marry  person  give  anti  matter  couple  relationship  personal  legal  birth_control  birth  planned_parenthood  provide\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic4\n",
      "fact  lie  claim  truth  true  reality  ignore  real  simply  admit  shame  matter  wrong  prove  public  trust  false  face  statement  honest  attempt  hold  call  refuse  excuse  pretend  hide  accept  action  doubt\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic5\n",
      "religious  religion  freedom  christian  church  god  belief  society  moral  world  culture  faith  practice  muslim  evil  pope  modern  catholic  respect  base  group  accept  true  principle  philosophy  islam  sin  follow  concept  form\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic6\n",
      "br  amp  http_www  target  href_http  title  article  news  www  http  org  opinion  blog  html_target  html_title  report  link  gov  www_nytime  find  story  post  html  accord  page  read  interesting  net  content  show\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic7\n",
      "school  student  college  education  high  teacher  public  learn  teach  university  class  test  degree  year  program  kid  graduate  professor  career  math  skill  attend  educate  work  major  academic  board  job  private  grade\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic8\n",
      "pay  tax  government  income  benefit  public  fund  cost  cut  program  service  federal  raise  money  taxpayer  social_security  provide  private  low  receive  budget  retirement  revenue  dollar  earn  spending  fee  welfare  spend  support\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic9\n",
      "drug  exercise  risk  body  healthy  brain  food  weight  disease  pain  eat  effect  study  drink  diet  fat  research  vaccine  calorie  smoke  problem  sugar  depression  heart  cancer  normal  cure  alcohol  condition  physical\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic10\n",
      "woman  man  male  female  young  wear  girl  gender  feel  boy  hair  body  sex  dress  sexual  age  face  find  ms  fashion  smile  shoe  clothe  suit  rape  lady  jenner  transgender  desire  difference\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic11\n",
      "high  number  increase  rate  low  population  large  level  average  small  result  growth  reduce  compare  percent  rise  standard  due  raise  measure  total  percentage  decline  drop  effect  fall  include  factor  inflation  grow\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic12\n",
      "thing  talk  guy  happen  stop  bad  real  start  hear  head  kind  stupid  folk  put  back  call  guess  pretty  expect  turn  smart  good  suppose  stuff  whatev  lot  sound  throw  sort  worry\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic13\n",
      "trump  candidate  republican  debate  run  gop  hillary  campaign  sander  bernie_sander  donald_trump  win  bernie  presidential  medium  clinton  fox  primary  supporter  biden  appeal  party  chance  nomination  hillary_clinton  politic  speak  establishment  poll  election\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic14\n",
      "good  make  bad  thing  idea  sense  point  great  mistake  hard  give  easy  reason  difference  hope  chance  decision  lot  agree  luck  perfect  pretty  put  happen  sound  part  difficult  mind  place  impossible\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic15\n",
      "market  china  price  economy  business  trade  free  economic  chinese  profit  sell  buy  corporation  product  industry  interest  corporate  world  stock  big  consumer  create  demand  government  investment  feed  financial  global  capital  capitalism\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic16\n",
      "read  article  time  comment  write  story  book  nyt  piece  author  reader  report  mention  writer  column  paper  new_york  interesting  news  publish  find  post  reading  ms  opinion  newspaper  reporter  editorial  journalist  cover\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic17\n",
      "love  great  hope  feel  life  heart  experience  share  bring  find  music  hear  wonderful  enjoy  mind  listen  eye  moment  art  beautiful  happy  friend  memory  amazing  learn  sound  remember  song  deep  dream\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic18\n",
      "care  health  cost  insurance  patient  medical  doctor  plan  system  provide  treatment  healthcare  hospital  aca  cover  obamacare  medicare  service  coverage  physician  good  company  therapist  practice  affordable  treat  premium  therapy  pay  sick\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic19\n",
      "issue  understand  important  concern  lack  situation  focus  behavior  address  part  discussion  experience  matter  personal  face  regard  difficult  attention  discuss  involve  approach  response  conversation  reaction  action  perspective  present  problem  agree  engage\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic20\n",
      "child  family  parent  kid  friend  life  young  age  live  mother  father  home  adult  grow  son  bear  daughter  wife  husband  generation  raise  single  brother  mom  love  sister  die  feel  dad  learn\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic21\n",
      "br  dd  colossal  gt_gt  albert_einstein  kalidan  prod  unsurprisingly  throwaway  ter  rot  gt  fell_apart  michael_bain  oar  naught  warp  feeble  abe_lincoln  glorieta_new  endlessly  permanently  lt_lt  zip  forewarn  unmitigated  crock  marge  undisputed  modicum\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic22\n",
      "problem  poor  system  rich  solution  people  economic  society  social  create  policy  class  real  middle_class  poverty  work  blame  solve  wealth  fix  lack  opportunity  wealthy  fail  address  continue  inequality  result  elite  part\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic23\n",
      "police  crime  criminal  case  officer  cop  prison  justice  charge  victim  commit  murder  jail  arrest  judge  abuse  system  stop  sentence  innocent  violent  shoot  guilty  behavior  trial  situation  kill  person  evidence  suspect\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic24\n",
      "water  land  grow  energy  oil  california  clean  produce  resource  environment  power  plant  air  source  waste  build  farm  farmer  wind  industry  large  big  gas  coal  environmental  area  supply  fuel  natural  sea\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic25\n",
      "make  food  add  eat  good  serve  recipe  easy  restaurant  cook  bit  great  taste  table  tip  cut  fresh  nice  chicken  meat  store  meal  minute  half  dinner  wine  delicious  hot  egg  top\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic26\n",
      "republican  vote  party  democrat  election  voter  democratic  support  political  politician  congress  democracy  gop  majority  voting  house  interest  elect  win  policy  represent  senate  member  representative  senator  issue  politic  poll  base  favor\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic27\n",
      "law  state  rule  court  case  legal  constitution  government  decision  act  require  texas  citizen  pass  federal  justice  lawyer  protect  judge  congress  apply  robert  decide  intent  process  constitutional  establish  set  enforce  order\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic28\n",
      "science  study  climate_change  climate  scientist  datum  change  research  show  evidence  base  model  rise  earth  scientific  warm  theory  global  temperature  find  record  planet  result  effect  expert  paper  trend  year  decade  analysis\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic29\n",
      "greece  greek  debt  bank  germany  europe  german  economy  country  government  loan  european  euro  economic  crisis  financial  money  austerity  pay  krugman  currency  reform  borrow  banker  default  creditor  leave  union  eurozone  demand\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic30\n",
      "br  give  call  free  speech  part  follow  include  list  today  watch  offer  line  hold  leave  true  org_wiki  en_wikipedia  target_https  title_https  href_https  entire  www_youtube  real  note  full  dear  break  rest  mention\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic31\n",
      "show  watch  tv  movie  character  funny  video  film  picture  miss  story  photo  real  laugh  audience  interview  interesting  scene  joke  episode  season  series  review  line  star  actor  television  news  king  reality\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic32\n",
      "drive  car  line  run  travel  walk  road  back  stop  mile  train  time  foot  front  put  horse  fly  driver  park  trip  open  check  wait  seat  close  ride  hit  jump  place  airline\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic33\n",
      "president  obama  bush  run  bill  office  state  elect  administration  policy  governor  clinton  walker  reagan  mr  great  fail  jeb  house  remember  presidency  legacy  promise  wisconsin  christie  rubio  florida  leader  senator  leadership\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic34\n",
      "black  white  race  american  group  people  community  african  racism  racist  culture  matter  racial  color  identify  person  minority  privilege  america  blow  identity  discrimination  cultural  society  difference  hispanic  individual  skin  asian  hate\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic35\n",
      "change  power  future  lead  step  continue  goal  individual  result  current  hope  end  achieve  effort  interest  strong  remain  order  seek  process  success  progress  political  forward  create  control  ability  challenge  path  influence\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic36\n",
      "information  security  internet  technology  service  phone  secret  email  computer  government  datum  personal  find  call  ad  access  send  public  account  record  check  apple  provide  online  search  site  privacy  private  agency  include\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic37\n",
      "money  make  give  big  spend  buy  dollar  time  million  put  save  lose  sell  worth  back  hand  lot  billion  huge  thousand  amount  waste  hundred  run  politician  real  cash  good  small  manage\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic38\n",
      "people  person  feel  make  thing  live  lot  understand  find  kind  hard  hurt  place  reason  wrong  realize  happen  change  talk  life  put  sad  happy  deserve  treat  matter  personally  simply  decent  respect\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic39\n",
      "war  fight  iraq  isis  military  force  attack  group  enemy  support  middle_east  terrorist  world  send  government  army  destroy  serve  country  syria  turkey  soldier  vietnam  troop  end  conflict  muslim  battle  threat  defeat\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic40\n",
      "day  work  time  week  hour  hard  leave  back  sit  night  wait  put  summer  long  stay  office  month  room  find  end  start  sleep  set  full  late  break  spend  early  turn  walk\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic41\n",
      "word  speak  hear  comment  find  language  refer  mind  guess  sound  describe  call  post  mention  puzzle  explain  fill  clue  point  bit  fit  term  note  thought  letter  today  kind  phrase  understand  agree\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic42\n",
      "state  history  flag  south  slavery  war  civil  today  southern  confederate_flag  slave  symbol  fight  represent  fly  remove  american  honor  stand  part  nation  hate  display  union  north  united_state  red  great  racist  proud\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic43\n",
      "play  game  team  sport  player  great  world  good  win  fan  football  field  big  soccer  match  ball  hit  head  final  athlete  watch  top  fifa  goal  club  run  beat  cup  time  professional\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic44\n",
      "year  time  long  ago  start  term  back  end  begin  happen  month  change  today  past  lose  decade  early  finally  continue  longer  move  period  short  late  remember  age  realize  leave  couple  wait\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic45\n",
      "conservative  liberal  medium  political  leave  side  wing  view  call  anti  hate  social  politic  mr  progressive  brook  idea  extreme  point  ideology  agenda  left  stewart  center  politician  radical  propaganda  public  daily  voice\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic46\n",
      "human  life  live  world  die  death  kill  animal  dog  suffer  nature  lion  end  save  survive  people  humanity  planet  hunt  destroy  specie  loss  dead  cat  understand  alive  protect  natural  human_being  cecil\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic47\n",
      "job  work  worker  company  pay  business  employee  wage  union  amazon  hire  labor  corporate  employer  ceo  disney  corporation  management  replace  benefit  salary  tech  customer  profit  low  skill  visa  minimum_wage  executive  employ\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic48\n",
      "american  country  america  world  nation  citizen  usa  immigrant  europe  united_state  illegal  immigration  border  mexico  foreign  european  india  canada  national  bear  refugee  africa  mexican  rest  french  bring  france  million  indian  native\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n",
      "Topic49\n",
      "live  city  place  move  home  area  house  build  new_york  local  neighborhood  building  housing  nyc  town  street  community  visit  rent  small  resident  neighbor  mayor  property  project  nice  leave  apartment  large  afford\n",
      "_____________________________________________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##a clearer rendering of topic words\n",
    "topics=[topic[1] for topic in model.show_topics(num_topics=50,num_words=30, formatted=False)]\n",
    "topwords=[]\n",
    "f=open(\"./plots/\"+str(50)+\"topicwords.csv\",\"w\")\n",
    "for topic in topics:\n",
    "    topicwords=[]\n",
    "    for topicword in topic:\n",
    "        topicwords.append(topicword[0])\n",
    "        f.write(topicword[0]+\",\")\n",
    "        topicstring=\"  \".join(topicwords)\n",
    "    f.write(\"\\n\")\n",
    "    topwords.append(topicstring)\n",
    "f.close()\n",
    "for idx, topword in enumerate(topwords):\n",
    "    print (\"Topic\"+str(idx)+\"\\n\"+topword+\"\\n_____________________________________________________________________________________________________________________\")\n",
    "    print ()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101\n",
      "1101\n"
     ]
    }
   ],
   "source": [
    "##vaxx files\n",
    "files = glob.glob('../documents/vaxx/*.txt')\n",
    "print(len(files))\n",
    "files=np.random.choice(files,int(len(files)))\n",
    "print(len(files))\n",
    "data = []\n",
    "for f in files:\n",
    "    with open(f, 'r') as fin:\n",
    "        data.append(fin.read())\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445441\n",
      "8908\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('../documents/comments/comments/*.txt')\n",
    "print(len(files))\n",
    "files=np.random.choice(files,int(len(files)/50))\n",
    "print(len(files))\n",
    "data = []\n",
    "for f in files:\n",
    "    with open(f, 'r') as fin:\n",
    "        data.append(fin.read())\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used to construct bigrams 6.80098295211792\n",
      "time used to construct Phrasors 10.101285219192505\n",
      "['you', 'didn', 'list', 'the', 'ultimate', 'and', 'most', 'significant', 'bit', 'of', 'subjective', 'opinion', 'that', 'cannot', 'be', 'extracted', 'from', 'journalism', 'the', 'simple', 'choice', 'of', 'what', 'gets', 'covered', 'choosing', 'to', 'shine', 'light', 'or', 'not', 'on', 'something', 'is', 'the', 'ultimate', 'subjectivity', 'and', 'every', 'outlet', 'does', 'it']\n",
      "time used to remove stopwords 2.7174110412597656\n",
      "time used to make bigrams 1.372959852218628\n",
      "time used to lemmatize words 81.59417605400085\n",
      "[['list', 'ultimate', 'significant', 'bit', 'subjective', 'opinion', 'can', 'not', 'extract', 'journalism', 'simple', 'choice', 'get', 'cover', 'choose', 'shine', 'light', 'something', 'ultimate', 'subjectivity', 'outlet']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "start=time.time()\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "end=time.time()\n",
    "print('time used to construct bigrams',end-start)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "start=time.time()\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "end=time.time()\n",
    "print('time used to construct Phrasors',end-start)\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    values=[delayed(nlp)(\" \".join(sent)) for sent in texts]\n",
    "    docs=compute(*values,scheduling='threads')\n",
    "    for doc in docs:\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "start=time.time()\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "end=time.time()\n",
    "print('time used to remove stopwords',end-start)\n",
    "# Form Bigrams\n",
    "start=time.time()\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "end=time.time()\n",
    "print('time used to make bigrams',end-start)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp.max_length = 9297925\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "start=time.time()\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "end=time.time()\n",
    "print('time used to lemmatize words',end-start)\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used 5.376482009887695\n",
      "([(35, 1), (97, 1), (125, 1), (505, 1), (562, 1), (776, 1), (814, 1), (842, 1), (1477, 1), (1487, 1), (1604, 1), (1966, 1), (3173, 1), (3767, 2), (6326, 1), (9229, 1), (12447, 1), (13112, 1), (16176, 1), (17562, 1)],)\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = mallet_model.id2word\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "start=time.time()\n",
    "values = [delayed(id2word.doc2bow)(text) for text in texts]\n",
    "corpus=compute(*values, scheduler='threads')\n",
    "end=time.time()\n",
    "print('time used', end-start)\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "X=np.asarray(model.inference(corpus)[0])\n",
    "num_topics=len(X[0])\n",
    "##normalize it\n",
    "X=(X.T/np.sum(X,axis=1)).T\n",
    "Y=np.asarray([len(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 11 17  0 14  2 11  6 25  1 32  4 29 25 39 13 27 49 29  1 43 36 47 32\n",
      " 21 31 35 35 43 22 31 11 49 45 31 37 41 31 41 29  2  1 25 42 37 42 26  8\n",
      "  7 18 33 15 18 25 18  5  4 25  9  8 49 23 27 16 36 28  6  5  9  0 29 20\n",
      " 12 30  2  0 11  2  9]\n",
      "neither\n",
      "[('deal', 0.06351775791253927), ('iran', 0.04667103855313569), ('israel', 0.024402029475718776), ('russia', 0.021640838021606327), ('agreement', 0.017171159355261797), ('nuclear', 0.01553170192938253), ('world', 0.014296068753667209), ('war', 0.01319849515065751), ('bomb', 0.011396817726849136), ('russian', 0.011362302833672731)]\n"
     ]
    }
   ],
   "source": [
    "##print((X[idx]))\n",
    "topics4X=np.argmax(X[1:80],axis=1)\n",
    "print(topics4X)\n",
    "idx=4\n",
    "topic4X=np.argmax(X[idx])\n",
    "print(\" \".join(data_words[idx]))\n",
    "print(model.show_topic(topic4X))\n",
    "##minor problem: the token br is not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##don't randomly run this. this will overwrite the mode. If you accidentally ran this, run the load function 3 blocks below\n",
    "normalized_model=Sequential()\n",
    "normalized_dense_layer=Dense(100,input_shape=[num_topics,],activation=\"relu\")\n",
    "normalized_dense_layer_2=Dense(1)\n",
    "normalized_model.add(normalized_dense_layer)\n",
    "normalized_model.add(normalized_dense_layer_2)\n",
    "normalized_model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['accuracy'])\n",
    "normalized_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.8770 - acc: 0.0480\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.8432 - acc: 0.0485\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 196.8208 - acc: 0.0482\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 196.7978 - acc: 0.0487\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.7722 - acc: 0.0487\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.7493 - acc: 0.0488\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.7274 - acc: 0.0487\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.7011 - acc: 0.0492\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.6808 - acc: 0.0486\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.6596 - acc: 0.0484\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.6311 - acc: 0.0494\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.6075 - acc: 0.0489\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.5913 - acc: 0.0493\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 196.5684 - acc: 0.0484\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.5410 - acc: 0.0489\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.5157 - acc: 0.0485\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.4954 - acc: 0.0486\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.4730 - acc: 0.0489\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.4457 - acc: 0.0489\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.4372 - acc: 0.0482\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 196.4086 - acc: 0.0475\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.3816 - acc: 0.0494\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.3659 - acc: 0.0495\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.3381 - acc: 0.0491\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.3134 - acc: 0.0498\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.2863 - acc: 0.0492\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.2745 - acc: 0.0494\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 196.2414 - acc: 0.0489\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.2158 - acc: 0.0489\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.1983 - acc: 0.0492\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.1737 - acc: 0.0493\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.1464 - acc: 0.0494\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 196.1384 - acc: 0.0492\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.1218 - acc: 0.0492\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 196.0809 - acc: 0.0497\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.0571 - acc: 0.0488\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 196.0428 - acc: 0.0486\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 196.0157 - acc: 0.0487\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 195.9842 - acc: 0.0492\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.9620 - acc: 0.0498\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.9544 - acc: 0.0483\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.9415 - acc: 0.0482\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 195.8947 - acc: 0.0493\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 195.8797 - acc: 0.0487\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.8791 - acc: 0.0484\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 195.8737 - acc: 0.0477\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 195.8065 - acc: 0.0493\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.7810 - acc: 0.0502\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.7587 - acc: 0.0500\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.7383 - acc: 0.0497\n",
      "training set(after 0 iterations) : 0.903\n",
      "1\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 195.7102 - acc: 0.0494\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 195.6899 - acc: 0.0486\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.6846 - acc: 0.0497\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.6465 - acc: 0.0494\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.6265 - acc: 0.0494\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 195.6075 - acc: 0.0494\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 195.5747 - acc: 0.0496\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 195.5546 - acc: 0.0504\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.5286 - acc: 0.0500\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.5063 - acc: 0.0497\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.4827 - acc: 0.0496\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.4610 - acc: 0.0505\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.4359 - acc: 0.0506\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.4184 - acc: 0.0502\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.3982 - acc: 0.0503\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.3704 - acc: 0.0505\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 195.3484 - acc: 0.0500\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 195.3331 - acc: 0.0503\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 195.3142 - acc: 0.0494\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.2792 - acc: 0.0496\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.2583 - acc: 0.0497\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.2328 - acc: 0.0503\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 195.2188 - acc: 0.0502\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.2027 - acc: 0.0501\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.1808 - acc: 0.0503\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.1578 - acc: 0.0509\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.1234 - acc: 0.0507\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 195.0984 - acc: 0.0504\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.0774 - acc: 0.0506\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 195.0551 - acc: 0.0501\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.0299 - acc: 0.0506\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 195.0152 - acc: 0.0505\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.9850 - acc: 0.0503\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.9597 - acc: 0.0506\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.9423 - acc: 0.0497\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.9203 - acc: 0.0495\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.8929 - acc: 0.0495\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.8727 - acc: 0.0500\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.8469 - acc: 0.0502\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.8238 - acc: 0.0502\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.8021 - acc: 0.0504\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.7901 - acc: 0.0507\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.7618 - acc: 0.0501\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.7345 - acc: 0.0504\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.7109 - acc: 0.0507\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.6878 - acc: 0.0505\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.6629 - acc: 0.0507\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 194.6402 - acc: 0.0502\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.6168 - acc: 0.0500\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.6158 - acc: 0.0503\n",
      "training set(after 50 iterations) : 0.904\n",
      "2\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.5716 - acc: 0.0505\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.5584 - acc: 0.0503\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.5434 - acc: 0.0498\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.5082 - acc: 0.0500\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.4800 - acc: 0.0501\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.4572 - acc: 0.0506\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.4346 - acc: 0.0504\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.4257 - acc: 0.0506\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.3862 - acc: 0.0503\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.3952 - acc: 0.0497\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 194.3621 - acc: 0.0502\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 194.3243 - acc: 0.0497\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.2998 - acc: 0.0497\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.2911 - acc: 0.0503\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.2613 - acc: 0.0500\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.2271 - acc: 0.0496\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.2102 - acc: 0.0501\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.1864 - acc: 0.0500\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 194.1815 - acc: 0.0498\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.1482 - acc: 0.0500\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.1143 - acc: 0.0504\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 194.0946 - acc: 0.0505\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.0659 - acc: 0.0500\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.0487 - acc: 0.0501\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 194.0313 - acc: 0.0495\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 194.0023 - acc: 0.0498\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.9870 - acc: 0.0498\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.9561 - acc: 0.0495\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.9385 - acc: 0.0503\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.9137 - acc: 0.0498\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.8853 - acc: 0.0496\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.8828 - acc: 0.0497\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.8480 - acc: 0.0504\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 193.8193 - acc: 0.0501\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 193.7994 - acc: 0.0501\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.7836 - acc: 0.0494\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.7514 - acc: 0.0501\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.7320 - acc: 0.0496\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.7026 - acc: 0.0496\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.6816 - acc: 0.0497\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.6640 - acc: 0.0495\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.6361 - acc: 0.0495\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.6185 - acc: 0.0502\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.6086 - acc: 0.0503\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.5661 - acc: 0.0496\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.5496 - acc: 0.0496\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.5267 - acc: 0.0498\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.4983 - acc: 0.0497\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.4749 - acc: 0.0489\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.4528 - acc: 0.0493\n",
      "training set(after 100 iterations) : 0.905\n",
      "3\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 193.4299 - acc: 0.0494\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.4134 - acc: 0.0496\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 193.3861 - acc: 0.0494\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.3612 - acc: 0.0500\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.3381 - acc: 0.0502\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 193.3157 - acc: 0.0502\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.2921 - acc: 0.0497\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.2746 - acc: 0.0493\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.2528 - acc: 0.0494\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.2235 - acc: 0.0494\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.2083 - acc: 0.0500\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.2029 - acc: 0.0496\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.1633 - acc: 0.0498\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.1381 - acc: 0.0500\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.1177 - acc: 0.0496\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 193.1136 - acc: 0.0502\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.0733 - acc: 0.0496\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.0511 - acc: 0.0500\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 193.0454 - acc: 0.0501\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 193.0079 - acc: 0.0505\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.9874 - acc: 0.0509\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.9591 - acc: 0.0501\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.9366 - acc: 0.0493\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.9143 - acc: 0.0492\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.8875 - acc: 0.0495\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.8598 - acc: 0.0497\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.8390 - acc: 0.0505\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 192.8223 - acc: 0.0505\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.7956 - acc: 0.0506\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.7709 - acc: 0.0505\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 192.7484 - acc: 0.0495\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.7249 - acc: 0.0498\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.7028 - acc: 0.0498\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.6774 - acc: 0.0495\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.6570 - acc: 0.0505\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.6417 - acc: 0.0504\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.6082 - acc: 0.0506\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.5881 - acc: 0.0494\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.5806 - acc: 0.0496\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.5433 - acc: 0.0494\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.5169 - acc: 0.0496\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.4945 - acc: 0.0509\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.4816 - acc: 0.0507\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.4553 - acc: 0.0513\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.4259 - acc: 0.0507\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.4039 - acc: 0.0492\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.3895 - acc: 0.0491\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.3622 - acc: 0.0494\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.3369 - acc: 0.0496\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.3162 - acc: 0.0496\n",
      "training set(after 150 iterations) : 0.905\n",
      "4\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.2923 - acc: 0.0504\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 192.2662 - acc: 0.0510\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.2433 - acc: 0.0511\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.2221 - acc: 0.0503\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.2005 - acc: 0.0498\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.1758 - acc: 0.0500\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.1562 - acc: 0.0494\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 192.1317 - acc: 0.0498\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.1061 - acc: 0.0494\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.0817 - acc: 0.0497\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.0575 - acc: 0.0509\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 192.0381 - acc: 0.0512\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 192.0118 - acc: 0.0509\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.9932 - acc: 0.0503\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.9729 - acc: 0.0495\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.9447 - acc: 0.0497\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.9209 - acc: 0.0498\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.8951 - acc: 0.0505\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.8720 - acc: 0.0506\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.8511 - acc: 0.0500\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.8372 - acc: 0.0507\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.8005 - acc: 0.0505\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.7926 - acc: 0.0489\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 191.7634 - acc: 0.0491\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.7342 - acc: 0.0503\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 191.7124 - acc: 0.0501\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.6854 - acc: 0.0502\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.6648 - acc: 0.0503\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.6436 - acc: 0.0502\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.6216 - acc: 0.0496\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 191.5955 - acc: 0.0498\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.5713 - acc: 0.0507\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.5525 - acc: 0.0503\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.5234 - acc: 0.0504\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.5039 - acc: 0.0514\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.4828 - acc: 0.0513\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.4597 - acc: 0.0502\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.4318 - acc: 0.0497\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.4113 - acc: 0.0500\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.3860 - acc: 0.0501\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.3775 - acc: 0.0494\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.3412 - acc: 0.0492\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 191.3116 - acc: 0.0496\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 191.3169 - acc: 0.0503\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.2780 - acc: 0.0512\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.2520 - acc: 0.0498\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 191.2260 - acc: 0.0501\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.1993 - acc: 0.0495\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.1883 - acc: 0.0502\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 191.1586 - acc: 0.0502\n",
      "training set(after 200 iterations) : 0.906\n",
      "5\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 191.1314 - acc: 0.0497\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.1093 - acc: 0.0495\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.0869 - acc: 0.0495\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.0628 - acc: 0.0496\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.0380 - acc: 0.0496\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 191.0264 - acc: 0.0495\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 191.0094 - acc: 0.0497\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.9746 - acc: 0.0502\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.9753 - acc: 0.0498\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.9318 - acc: 0.0496\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.8981 - acc: 0.0497\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.8774 - acc: 0.0497\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.8557 - acc: 0.0505\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.8353 - acc: 0.0506\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.8155 - acc: 0.0498\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.7893 - acc: 0.0494\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 190.7614 - acc: 0.0495\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.7369 - acc: 0.0495\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 190.7124 - acc: 0.0497\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.6995 - acc: 0.0504\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.6789 - acc: 0.0505\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.6434 - acc: 0.0495\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.6219 - acc: 0.0494\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.6075 - acc: 0.0496\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.5848 - acc: 0.0494\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.5594 - acc: 0.0496\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.5288 - acc: 0.0494\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.5061 - acc: 0.0487\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.4816 - acc: 0.0489\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.4593 - acc: 0.0489\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.4370 - acc: 0.0488\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.4115 - acc: 0.0494\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.3952 - acc: 0.0502\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.3714 - acc: 0.0497\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 190.3477 - acc: 0.0496\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 190.3228 - acc: 0.0497\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.2984 - acc: 0.0491\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 190.3015 - acc: 0.0488\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.2511 - acc: 0.0488\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.2529 - acc: 0.0488\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.2120 - acc: 0.0493\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.1829 - acc: 0.0497\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.1737 - acc: 0.0497\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.1581 - acc: 0.0506\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.1217 - acc: 0.0495\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.1043 - acc: 0.0495\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.0716 - acc: 0.0491\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.0479 - acc: 0.0487\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.0287 - acc: 0.0489\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 190.0027 - acc: 0.0491\n",
      "training set(after 250 iterations) : 0.906\n",
      "6\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.9806 - acc: 0.0494\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.9607 - acc: 0.0494\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.9360 - acc: 0.0492\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.9140 - acc: 0.0488\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.8906 - acc: 0.0485\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.8671 - acc: 0.0491\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.8433 - acc: 0.0495\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.8223 - acc: 0.0495\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.7999 - acc: 0.0492\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.7832 - acc: 0.0491\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.7566 - acc: 0.0486\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.7368 - acc: 0.0496\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 189.7162 - acc: 0.0493\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.6875 - acc: 0.0486\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 189.6637 - acc: 0.0491\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.6399 - acc: 0.0497\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.6347 - acc: 0.0500\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.6049 - acc: 0.0495\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.5758 - acc: 0.0494\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.5693 - acc: 0.0492\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.5290 - acc: 0.0489\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.5084 - acc: 0.0484\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.4944 - acc: 0.0495\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.4756 - acc: 0.0488\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.4424 - acc: 0.0487\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.4130 - acc: 0.0497\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.3982 - acc: 0.0501\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.3773 - acc: 0.0496\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.3487 - acc: 0.0503\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.3267 - acc: 0.0493\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.3070 - acc: 0.0488\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 189.2925 - acc: 0.0484\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.2600 - acc: 0.0489\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.2343 - acc: 0.0492\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.2202 - acc: 0.0497\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.1980 - acc: 0.0498\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 189.1695 - acc: 0.0501\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 189.1457 - acc: 0.0497\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.1277 - acc: 0.0492\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.1142 - acc: 0.0495\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.0775 - acc: 0.0494\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.0555 - acc: 0.0496\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 189.0345 - acc: 0.0495\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 189.0096 - acc: 0.0501\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.9895 - acc: 0.0497\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.9719 - acc: 0.0497\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.9404 - acc: 0.0495\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.9250 - acc: 0.0495\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 188.8987 - acc: 0.0498\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 188.8760 - acc: 0.0496\n",
      "training set(after 300 iterations) : 0.907\n",
      "7\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 188.8518 - acc: 0.0494\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.8336 - acc: 0.0496\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.8098 - acc: 0.0498\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.7859 - acc: 0.0498\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 188.7664 - acc: 0.0496\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.7405 - acc: 0.0495\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.7194 - acc: 0.0495\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.6957 - acc: 0.0496\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.6739 - acc: 0.0500\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.6523 - acc: 0.0498\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.6307 - acc: 0.0496\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.6069 - acc: 0.0495\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.5830 - acc: 0.0495\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.5622 - acc: 0.0496\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 188.5387 - acc: 0.0494\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.5170 - acc: 0.0491\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.4959 - acc: 0.0491\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.4737 - acc: 0.0494\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.4528 - acc: 0.0492\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.4286 - acc: 0.0493\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.4054 - acc: 0.0493\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.3908 - acc: 0.0491\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.3676 - acc: 0.0491\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.3395 - acc: 0.0488\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.3260 - acc: 0.0492\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.3009 - acc: 0.0488\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.2842 - acc: 0.0493\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.2578 - acc: 0.0489\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 188.2353 - acc: 0.0491\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.2314 - acc: 0.0489\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 188.1951 - acc: 0.0487\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 188.1658 - acc: 0.0493\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.1489 - acc: 0.0484\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.1334 - acc: 0.0498\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.1057 - acc: 0.0488\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.0937 - acc: 0.0488\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.0655 - acc: 0.0487\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 188.0394 - acc: 0.0493\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 188.0134 - acc: 0.0489\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.9939 - acc: 0.0486\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.9784 - acc: 0.0489\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.9508 - acc: 0.0488\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.9254 - acc: 0.0489\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.9078 - acc: 0.0495\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.8832 - acc: 0.0492\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.8639 - acc: 0.0495\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.8386 - acc: 0.0494\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.8191 - acc: 0.0486\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.8071 - acc: 0.0491\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.7767 - acc: 0.0482\n",
      "training set(after 350 iterations) : 0.908\n",
      "8\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.7561 - acc: 0.0482\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.7328 - acc: 0.0486\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.7111 - acc: 0.0484\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.6890 - acc: 0.0486\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.6760 - acc: 0.0489\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.6544 - acc: 0.0491\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.6205 - acc: 0.0497\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.6028 - acc: 0.0493\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.5919 - acc: 0.0487\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.5583 - acc: 0.0493\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.5324 - acc: 0.0486\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 187.5147 - acc: 0.0488\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.4949 - acc: 0.0487\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.4733 - acc: 0.0486\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.4491 - acc: 0.0487\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.4287 - acc: 0.0487\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 187.4079 - acc: 0.0488\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.3821 - acc: 0.0488\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.3570 - acc: 0.0493\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.3445 - acc: 0.0485\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.3329 - acc: 0.0487\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.3020 - acc: 0.0488\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.2712 - acc: 0.0484\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.2485 - acc: 0.0487\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.2340 - acc: 0.0492\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.2136 - acc: 0.0491\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.1826 - acc: 0.0487\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.1860 - acc: 0.0492\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.1500 - acc: 0.0487\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.1394 - acc: 0.0491\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.1020 - acc: 0.0488\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.0818 - acc: 0.0488\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 187.0537 - acc: 0.0487\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.0282 - acc: 0.0491\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 187.0277 - acc: 0.0486\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.9966 - acc: 0.0491\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.9659 - acc: 0.0485\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.9430 - acc: 0.0484\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.9296 - acc: 0.0483\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.8979 - acc: 0.0484\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 186.8740 - acc: 0.0486\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.8666 - acc: 0.0491\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.8402 - acc: 0.0487\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 186.8142 - acc: 0.0488\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 186.8033 - acc: 0.0485\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 186.7834 - acc: 0.0486\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.7535 - acc: 0.0488\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 186.7350 - acc: 0.0493\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 186.7085 - acc: 0.0491\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.6892 - acc: 0.0489\n",
      "training set(after 400 iterations) : 0.908\n",
      "9\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.6744 - acc: 0.0492\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.6394 - acc: 0.0480\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.6239 - acc: 0.0479\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.5984 - acc: 0.0479\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.5732 - acc: 0.0479\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.5533 - acc: 0.0486\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.5331 - acc: 0.0486\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.5129 - acc: 0.0491\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.5073 - acc: 0.0496\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.4757 - acc: 0.0488\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.4556 - acc: 0.0489\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.4327 - acc: 0.0486\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.4103 - acc: 0.0480\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.3909 - acc: 0.0486\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.3697 - acc: 0.0488\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.3571 - acc: 0.0483\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.3231 - acc: 0.0489\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.3043 - acc: 0.0489\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.2812 - acc: 0.0488\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.2557 - acc: 0.0486\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.2362 - acc: 0.0482\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.2121 - acc: 0.0484\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.1913 - acc: 0.0489\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.1669 - acc: 0.0487\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.1524 - acc: 0.0492\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - ETA: 0s - loss: 184.0569 - acc: 0.049 - 0s 3us/step - loss: 186.1275 - acc: 0.0492\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.1079 - acc: 0.0491\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 186.0842 - acc: 0.0488\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.0692 - acc: 0.0488\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.0395 - acc: 0.0489\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 186.0194 - acc: 0.0485\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.9945 - acc: 0.0492\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 185.9783 - acc: 0.0492\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.9620 - acc: 0.0492\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.9336 - acc: 0.0485\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.9206 - acc: 0.0488\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.8950 - acc: 0.0493\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.8723 - acc: 0.0486\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.8609 - acc: 0.0491\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.8244 - acc: 0.0488\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.8163 - acc: 0.0486\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.7918 - acc: 0.0486\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.7624 - acc: 0.0483\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.7430 - acc: 0.0486\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.7257 - acc: 0.0487\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.6995 - acc: 0.0487\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.6758 - acc: 0.0482\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.6779 - acc: 0.0484\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.6396 - acc: 0.0483\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.6167 - acc: 0.0484\n",
      "training set(after 450 iterations) : 0.909\n",
      "10\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 185.5988 - acc: 0.0488\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.5732 - acc: 0.0482\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.5486 - acc: 0.0480\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.5282 - acc: 0.0488\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 185.5255 - acc: 0.0489\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.4900 - acc: 0.0488\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.4662 - acc: 0.0477\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.4463 - acc: 0.0478\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.4318 - acc: 0.0483\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.4017 - acc: 0.0482\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.3825 - acc: 0.0480\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.3727 - acc: 0.0494\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.3472 - acc: 0.0492\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 185.3214 - acc: 0.0485\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.3043 - acc: 0.0475\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.2856 - acc: 0.0488\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.2602 - acc: 0.0484\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.2348 - acc: 0.0480\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.2105 - acc: 0.0485\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.2070 - acc: 0.0485\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.1764 - acc: 0.0487\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.1550 - acc: 0.0485\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.1283 - acc: 0.0491\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.1075 - acc: 0.0480\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.0920 - acc: 0.0480\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.0680 - acc: 0.0483\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.0418 - acc: 0.0475\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 185.0317 - acc: 0.0482\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 185.0100 - acc: 0.0484\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.9804 - acc: 0.0479\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.9662 - acc: 0.0477\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.9376 - acc: 0.0473\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.9154 - acc: 0.0477\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.9019 - acc: 0.0478\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.8829 - acc: 0.0477\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.8526 - acc: 0.0476\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.8453 - acc: 0.0485\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.8177 - acc: 0.0487\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.7933 - acc: 0.0484\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.7702 - acc: 0.0478\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 184.7544 - acc: 0.0483\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.7328 - acc: 0.0480\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 184.7224 - acc: 0.0476\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.7002 - acc: 0.0480\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 184.6659 - acc: 0.0476\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.6489 - acc: 0.0477\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.6265 - acc: 0.0482\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.6076 - acc: 0.0480\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.5845 - acc: 0.0480\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.5634 - acc: 0.0475\n",
      "training set(after 500 iterations) : 0.909\n",
      "11\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 184.5438 - acc: 0.0479\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 184.5280 - acc: 0.0482\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.5104 - acc: 0.0479\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.4811 - acc: 0.0475\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.4695 - acc: 0.0479\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.4431 - acc: 0.0486\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.4155 - acc: 0.0482\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.3925 - acc: 0.0478\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.3802 - acc: 0.0477\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.3592 - acc: 0.0477\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.3464 - acc: 0.0480\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.3142 - acc: 0.0477\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.2891 - acc: 0.0482\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 184.2886 - acc: 0.0484\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 184.2587 - acc: 0.0492\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.2246 - acc: 0.0474\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.2118 - acc: 0.0484\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.1986 - acc: 0.0486\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.1755 - acc: 0.0485\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.1466 - acc: 0.0484\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 184.1377 - acc: 0.0484\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.1198 - acc: 0.0487\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.1081 - acc: 0.0477\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.0635 - acc: 0.0479\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 184.0464 - acc: 0.0478\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 184.0254 - acc: 0.0478\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 184.0039 - acc: 0.0479\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.9813 - acc: 0.0482\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.9599 - acc: 0.0479\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.9395 - acc: 0.0478\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.9205 - acc: 0.0479\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.8985 - acc: 0.0479\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.8765 - acc: 0.0477\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.8571 - acc: 0.0480\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 183.8361 - acc: 0.0482\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.8183 - acc: 0.0482\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.7980 - acc: 0.0483\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 183.7789 - acc: 0.0479\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.7567 - acc: 0.0479\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.7452 - acc: 0.0483\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.7104 - acc: 0.0482\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.6927 - acc: 0.0479\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.6677 - acc: 0.0482\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.6506 - acc: 0.0482\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.6351 - acc: 0.0479\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 183.6124 - acc: 0.0480\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.5890 - acc: 0.0480\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.5697 - acc: 0.0478\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.5547 - acc: 0.0479\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.5311 - acc: 0.0479\n",
      "training set(after 550 iterations) : 0.910\n",
      "12\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.5159 - acc: 0.0483\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.4897 - acc: 0.0480\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.4681 - acc: 0.0473\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 183.4594 - acc: 0.0480\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.4300 - acc: 0.0478\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.4300 - acc: 0.0477\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.3944 - acc: 0.0477\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.3670 - acc: 0.0485\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 183.3516 - acc: 0.0480\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 183.3331 - acc: 0.0483\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.3007 - acc: 0.0480\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.2813 - acc: 0.0480\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.2709 - acc: 0.0478\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.2514 - acc: 0.0485\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.2195 - acc: 0.0485\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.2101 - acc: 0.0480\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.1844 - acc: 0.0477\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - ETA: 0s - loss: 187.8222 - acc: 0.045 - 0s 3us/step - loss: 183.1708 - acc: 0.0478\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.1373 - acc: 0.0480\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.1296 - acc: 0.0483\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.0975 - acc: 0.0480\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.0827 - acc: 0.0482\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 183.0675 - acc: 0.0478\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.0371 - acc: 0.0483\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 183.0184 - acc: 0.0478\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.9937 - acc: 0.0483\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 182.9759 - acc: 0.0486\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 182.9555 - acc: 0.0487\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 182.9302 - acc: 0.0480\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.9257 - acc: 0.0482\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.9008 - acc: 0.0483\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.8742 - acc: 0.0486\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.8549 - acc: 0.0484\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.8322 - acc: 0.0483\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.8231 - acc: 0.0484\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.7964 - acc: 0.0484\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.7692 - acc: 0.0480\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 182.7547 - acc: 0.0479\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 182.7379 - acc: 0.0478\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.7176 - acc: 0.0479\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.6962 - acc: 0.0485\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.6737 - acc: 0.0482\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.6683 - acc: 0.0486\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.6391 - acc: 0.0486\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.6123 - acc: 0.0489\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.5936 - acc: 0.0489\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.5716 - acc: 0.0488\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.5532 - acc: 0.0489\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.5321 - acc: 0.0487\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.5171 - acc: 0.0484\n",
      "training set(after 600 iterations) : 0.910\n",
      "13\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 182.4954 - acc: 0.0477\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.4737 - acc: 0.0488\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.4512 - acc: 0.0487\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.4310 - acc: 0.0488\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.4128 - acc: 0.0487\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.3929 - acc: 0.0492\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.3748 - acc: 0.0487\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.3542 - acc: 0.0487\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.3389 - acc: 0.0495\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.3249 - acc: 0.0486\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.2919 - acc: 0.0488\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.2786 - acc: 0.0488\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.2554 - acc: 0.0488\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.2340 - acc: 0.0488\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.2126 - acc: 0.0482\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.1981 - acc: 0.0482\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.1708 - acc: 0.0485\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.1566 - acc: 0.0494\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.1366 - acc: 0.0487\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.1291 - acc: 0.0489\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.0913 - acc: 0.0491\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.0827 - acc: 0.0486\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 182.0648 - acc: 0.0474\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.0383 - acc: 0.0474\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.0143 - acc: 0.0485\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 182.0033 - acc: 0.0489\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.9795 - acc: 0.0492\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.9551 - acc: 0.0492\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 181.9516 - acc: 0.0482\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.9264 - acc: 0.0482\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.8998 - acc: 0.0480\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.9261 - acc: 0.0480\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.8579 - acc: 0.0494\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.8371 - acc: 0.0477\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.8179 - acc: 0.0476\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.8078 - acc: 0.0480\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 181.7878 - acc: 0.0485\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.7589 - acc: 0.0492\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.7561 - acc: 0.0489\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.7157 - acc: 0.0486\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.6945 - acc: 0.0492\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.6758 - acc: 0.0492\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.6646 - acc: 0.0491\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.6376 - acc: 0.0483\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 181.6178 - acc: 0.0482\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.5945 - acc: 0.0486\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.5920 - acc: 0.0488\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.5634 - acc: 0.0493\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.5469 - acc: 0.0491\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.5197 - acc: 0.0484\n",
      "training set(after 650 iterations) : 0.911\n",
      "14\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.5091 - acc: 0.0486\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.4825 - acc: 0.0485\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.4621 - acc: 0.0486\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.4460 - acc: 0.0487\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.4305 - acc: 0.0475\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.3997 - acc: 0.0478\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.3786 - acc: 0.0487\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 181.3642 - acc: 0.0488\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.3526 - acc: 0.0486\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.3218 - acc: 0.0484\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.3186 - acc: 0.0483\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.2997 - acc: 0.0478\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.2641 - acc: 0.0479\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 181.2445 - acc: 0.0486\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.2305 - acc: 0.0491\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.2174 - acc: 0.0492\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.1875 - acc: 0.0487\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.1679 - acc: 0.0478\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.1450 - acc: 0.0479\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.1276 - acc: 0.0484\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.1051 - acc: 0.0483\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.0882 - acc: 0.0484\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.0657 - acc: 0.0484\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 181.0491 - acc: 0.0484\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 181.0324 - acc: 0.0476\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 181.0089 - acc: 0.0474\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.9861 - acc: 0.0477\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.9726 - acc: 0.0482\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.9507 - acc: 0.0491\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.9283 - acc: 0.0492\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 180.9077 - acc: 0.0487\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.8844 - acc: 0.0480\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.8712 - acc: 0.0484\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.8454 - acc: 0.0483\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 180.8263 - acc: 0.0479\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.8141 - acc: 0.0475\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 180.7910 - acc: 0.0479\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.7633 - acc: 0.0484\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.7485 - acc: 0.0483\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 180.7243 - acc: 0.0484\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.7071 - acc: 0.0480\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.6864 - acc: 0.0479\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.6656 - acc: 0.0476\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.6467 - acc: 0.0487\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 180.6328 - acc: 0.0487\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.6064 - acc: 0.0485\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.5855 - acc: 0.0479\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.5670 - acc: 0.0480\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.5448 - acc: 0.0482\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.5251 - acc: 0.0475\n",
      "training set(after 700 iterations) : 0.911\n",
      "15\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 180.5103 - acc: 0.0476\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.4878 - acc: 0.0478\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.4650 - acc: 0.0478\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.4417 - acc: 0.0480\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 180.4360 - acc: 0.0477\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.4062 - acc: 0.0487\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 180.3883 - acc: 0.0486\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 180.3675 - acc: 0.0484\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.3430 - acc: 0.0478\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.3250 - acc: 0.0482\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.3092 - acc: 0.0478\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.2913 - acc: 0.0482\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.2621 - acc: 0.0483\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.2449 - acc: 0.0484\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.2303 - acc: 0.0479\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.2140 - acc: 0.0480\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.1878 - acc: 0.0477\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.1718 - acc: 0.0487\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.1493 - acc: 0.0482\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.1432 - acc: 0.0489\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.1151 - acc: 0.0482\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.0957 - acc: 0.0485\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.0692 - acc: 0.0487\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.0501 - acc: 0.0482\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.0281 - acc: 0.0479\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 180.0332 - acc: 0.0485\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.9962 - acc: 0.0485\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.9784 - acc: 0.0477\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.9531 - acc: 0.0483\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.9335 - acc: 0.0485\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.9184 - acc: 0.0482\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.8908 - acc: 0.0482\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.8731 - acc: 0.0480\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.8595 - acc: 0.0479\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.8426 - acc: 0.0483\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.8144 - acc: 0.0480\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 179.7946 - acc: 0.0480\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.7762 - acc: 0.0486\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.7576 - acc: 0.0487\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.7433 - acc: 0.0491\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 179.7156 - acc: 0.0482\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.7318 - acc: 0.0478\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.6786 - acc: 0.0480\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.6588 - acc: 0.0485\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.6449 - acc: 0.0494\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.6205 - acc: 0.0485\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 179.5990 - acc: 0.0485\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.5813 - acc: 0.0479\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.5660 - acc: 0.0480\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.5501 - acc: 0.0482\n",
      "training set(after 750 iterations) : 0.912\n",
      "16\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.5233 - acc: 0.0483\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.5097 - acc: 0.0496\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.4915 - acc: 0.0500\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.4623 - acc: 0.0492\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.4461 - acc: 0.0486\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.4547 - acc: 0.0477\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.4178 - acc: 0.0482\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.3894 - acc: 0.0487\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.3759 - acc: 0.0498\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.3581 - acc: 0.0489\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.3320 - acc: 0.0488\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.3093 - acc: 0.0484\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.3134 - acc: 0.0486\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.2891 - acc: 0.0483\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.2533 - acc: 0.0480\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 179.2318 - acc: 0.0486\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.2253 - acc: 0.0492\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.1948 - acc: 0.0492\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 179.1803 - acc: 0.0494\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 179.1533 - acc: 0.0487\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.1466 - acc: 0.0485\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.1237 - acc: 0.0487\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.1012 - acc: 0.0487\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.0849 - acc: 0.0491\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.0611 - acc: 0.0489\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 179.0440 - acc: 0.0494\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 179.0191 - acc: 0.0491\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 179.0027 - acc: 0.0488\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.9847 - acc: 0.0491\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.9630 - acc: 0.0491\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.9446 - acc: 0.0492\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.9269 - acc: 0.0491\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.9232 - acc: 0.0486\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 178.8915 - acc: 0.0487\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.8628 - acc: 0.0493\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.8512 - acc: 0.0494\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.8303 - acc: 0.0497\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.8099 - acc: 0.0496\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.8046 - acc: 0.0493\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.7670 - acc: 0.0493\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 178.7495 - acc: 0.0491\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 178.7334 - acc: 0.0489\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.7138 - acc: 0.0492\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.6925 - acc: 0.0488\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.6775 - acc: 0.0485\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.6538 - acc: 0.0487\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.6376 - acc: 0.0492\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.6166 - acc: 0.0491\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.5981 - acc: 0.0493\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.5781 - acc: 0.0492\n",
      "training set(after 800 iterations) : 0.912\n",
      "17\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.5697 - acc: 0.0485\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.5384 - acc: 0.0486\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.5178 - acc: 0.0488\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 178.5069 - acc: 0.0488\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.4833 - acc: 0.0486\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.4648 - acc: 0.0487\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.4460 - acc: 0.0489\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.4274 - acc: 0.0491\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.4133 - acc: 0.0488\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.3910 - acc: 0.0492\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.3830 - acc: 0.0493\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.3573 - acc: 0.0494\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.3627 - acc: 0.0487\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 178.3238 - acc: 0.0476\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.2911 - acc: 0.0478\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.2718 - acc: 0.0486\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.2616 - acc: 0.0496\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.2427 - acc: 0.0497\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.2282 - acc: 0.0485\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.2170 - acc: 0.0483\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.1863 - acc: 0.0480\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.1579 - acc: 0.0489\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.1413 - acc: 0.0493\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.1283 - acc: 0.0495\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.1068 - acc: 0.0497\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.0832 - acc: 0.0486\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.0677 - acc: 0.0482\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.0493 - acc: 0.0480\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 178.0312 - acc: 0.0482\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.0138 - acc: 0.0488\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 178.0111 - acc: 0.0491\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 178.0064 - acc: 0.0509\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.9658 - acc: 0.0502\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.9365 - acc: 0.0484\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 177.9348 - acc: 0.0468\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.9035 - acc: 0.0482\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.8804 - acc: 0.0485\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.8639 - acc: 0.0500\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.8485 - acc: 0.0500\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.8236 - acc: 0.0493\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.8036 - acc: 0.0492\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.7871 - acc: 0.0486\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.7770 - acc: 0.0479\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.7526 - acc: 0.0482\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.7265 - acc: 0.0488\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.7114 - acc: 0.0495\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.6946 - acc: 0.0504\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.6729 - acc: 0.0503\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 177.6618 - acc: 0.0486\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.6410 - acc: 0.0483\n",
      "training set(after 850 iterations) : 0.913\n",
      "18\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.6199 - acc: 0.0486\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.5941 - acc: 0.0488\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.5834 - acc: 0.0496\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.5789 - acc: 0.0503\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.5573 - acc: 0.0498\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.5487 - acc: 0.0485\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.5164 - acc: 0.0487\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.4876 - acc: 0.0492\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.4678 - acc: 0.0503\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.4572 - acc: 0.0500\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.4384 - acc: 0.0498\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.4116 - acc: 0.0500\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.3973 - acc: 0.0479\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.3886 - acc: 0.0491\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.3667 - acc: 0.0486\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.3386 - acc: 0.0488\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.3221 - acc: 0.0500\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.3164 - acc: 0.0504\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.3022 - acc: 0.0504\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.2660 - acc: 0.0501\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.2510 - acc: 0.0483\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.2537 - acc: 0.0485\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.2300 - acc: 0.0487\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.1907 - acc: 0.0498\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.1808 - acc: 0.0505\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.1638 - acc: 0.0501\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.1580 - acc: 0.0506\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.1174 - acc: 0.0504\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 177.1028 - acc: 0.0498\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.0876 - acc: 0.0491\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 177.0661 - acc: 0.0489\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.0530 - acc: 0.0486\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.0357 - acc: 0.0496\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 177.0084 - acc: 0.0507\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.9906 - acc: 0.0505\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.9738 - acc: 0.0500\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.9689 - acc: 0.0486\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.9366 - acc: 0.0493\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.9214 - acc: 0.0501\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.9019 - acc: 0.0504\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.8831 - acc: 0.0502\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.8668 - acc: 0.0493\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.8519 - acc: 0.0494\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.8300 - acc: 0.0494\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.8105 - acc: 0.0495\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.7899 - acc: 0.0495\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.7745 - acc: 0.0503\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.7606 - acc: 0.0510\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.7385 - acc: 0.0507\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.7189 - acc: 0.0505\n",
      "training set(after 900 iterations) : 0.913\n",
      "19\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 176.7003 - acc: 0.0501\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 176.7036 - acc: 0.0497\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.6629 - acc: 0.0507\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.6474 - acc: 0.0503\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.6259 - acc: 0.0504\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.6217 - acc: 0.0501\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.5933 - acc: 0.0504\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.5720 - acc: 0.0503\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.5591 - acc: 0.0509\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.5396 - acc: 0.0506\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.5171 - acc: 0.0505\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.5379 - acc: 0.0496\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.4883 - acc: 0.0495\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.4680 - acc: 0.0504\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.4523 - acc: 0.0510\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.4334 - acc: 0.0510\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.4169 - acc: 0.0505\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.4027 - acc: 0.0502\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.3753 - acc: 0.0501\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.3602 - acc: 0.0501\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.3384 - acc: 0.0503\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.3230 - acc: 0.0503\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.3028 - acc: 0.0505\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.2830 - acc: 0.0510\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.2687 - acc: 0.0509\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 176.2486 - acc: 0.0509\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 176.2358 - acc: 0.0507\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.2201 - acc: 0.0504\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 176.1955 - acc: 0.0503\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.1857 - acc: 0.0504\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.1717 - acc: 0.0506\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.1436 - acc: 0.0503\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.1306 - acc: 0.0504\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.1065 - acc: 0.0502\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.0937 - acc: 0.0504\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.0831 - acc: 0.0506\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.0567 - acc: 0.0505\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 176.0355 - acc: 0.0506\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 176.0188 - acc: 0.0506\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.9969 - acc: 0.0505\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.9842 - acc: 0.0509\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.9646 - acc: 0.0510\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 175.9433 - acc: 0.0507\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.9267 - acc: 0.0504\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.9381 - acc: 0.0510\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.8923 - acc: 0.0503\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.8725 - acc: 0.0507\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.8566 - acc: 0.0507\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 175.8521 - acc: 0.0510\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.8261 - acc: 0.0507\n",
      "training set(after 950 iterations) : 0.914\n",
      "20\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 175.8143 - acc: 0.0507\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.7985 - acc: 0.0506\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.7643 - acc: 0.0505\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.7605 - acc: 0.0503\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.7470 - acc: 0.0498\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.7184 - acc: 0.0502\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.6942 - acc: 0.0504\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.6918 - acc: 0.0507\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.6713 - acc: 0.0511\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.6546 - acc: 0.0507\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.6255 - acc: 0.0511\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.6174 - acc: 0.0504\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.5948 - acc: 0.0509\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.5734 - acc: 0.0505\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.5562 - acc: 0.0505\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 175.5432 - acc: 0.0505\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.5298 - acc: 0.0506\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.5105 - acc: 0.0514\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 175.4886 - acc: 0.0511\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.4708 - acc: 0.0510\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.4515 - acc: 0.0503\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 175.4340 - acc: 0.0506\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.4310 - acc: 0.0514\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.3997 - acc: 0.0507\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.3800 - acc: 0.0505\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.3578 - acc: 0.0509\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.3387 - acc: 0.0504\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.3308 - acc: 0.0507\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.3121 - acc: 0.0506\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.2852 - acc: 0.0503\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.2734 - acc: 0.0513\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.2648 - acc: 0.0501\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.2419 - acc: 0.0507\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 175.2203 - acc: 0.0510\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.2083 - acc: 0.0512\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.1866 - acc: 0.0509\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.1689 - acc: 0.0506\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 175.1484 - acc: 0.0513\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.1429 - acc: 0.0505\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.1293 - acc: 0.0509\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 175.1057 - acc: 0.0510\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 175.0769 - acc: 0.0511\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.0609 - acc: 0.0518\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.0401 - acc: 0.0510\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 175.0244 - acc: 0.0514\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 175.0065 - acc: 0.0515\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.9904 - acc: 0.0506\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.9924 - acc: 0.0511\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.9534 - acc: 0.0505\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.9433 - acc: 0.0513\n",
      "training set(after 1000 iterations) : 0.914\n",
      "21\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.9295 - acc: 0.0511\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.9081 - acc: 0.0513\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.8774 - acc: 0.0516\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.8689 - acc: 0.0513\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 174.8609 - acc: 0.0525\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.8419 - acc: 0.0512\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 174.8147 - acc: 0.0502\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 174.8022 - acc: 0.0514\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.7816 - acc: 0.0507\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 174.7555 - acc: 0.0507\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.7425 - acc: 0.0510\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.7361 - acc: 0.0523\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.7115 - acc: 0.0525\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.6819 - acc: 0.0506\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.6802 - acc: 0.0512\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.6697 - acc: 0.0510\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.6413 - acc: 0.0510\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.6151 - acc: 0.0513\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.6056 - acc: 0.0524\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.5896 - acc: 0.0522\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.5635 - acc: 0.0514\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.5448 - acc: 0.0513\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.5306 - acc: 0.0507\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.5191 - acc: 0.0506\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.4912 - acc: 0.0507\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.4779 - acc: 0.0511\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.4547 - acc: 0.0507\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.4507 - acc: 0.0511\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.4200 - acc: 0.0513\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.4100 - acc: 0.0515\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.3871 - acc: 0.0520\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 174.3669 - acc: 0.0518\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - ETA: 0s - loss: 182.0464 - acc: 0.047 - 0s 3us/step - loss: 174.3481 - acc: 0.0518\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 174.3340 - acc: 0.0507\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.3113 - acc: 0.0507\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.2983 - acc: 0.0514\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.2826 - acc: 0.0514\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 174.2640 - acc: 0.0516\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.2470 - acc: 0.0512\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.2227 - acc: 0.0510\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.2095 - acc: 0.0519\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.1885 - acc: 0.0515\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.1747 - acc: 0.0512\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.1661 - acc: 0.0513\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.1361 - acc: 0.0516\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.1271 - acc: 0.0512\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.1048 - acc: 0.0520\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.0821 - acc: 0.0514\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.0694 - acc: 0.0512\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 174.0503 - acc: 0.0522\n",
      "training set(after 1050 iterations) : 0.915\n",
      "22\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.0628 - acc: 0.0523\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.0181 - acc: 0.0519\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.9979 - acc: 0.0509\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 174.0085 - acc: 0.0512\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.9644 - acc: 0.0528\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.9463 - acc: 0.0518\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.9503 - acc: 0.0523\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.9238 - acc: 0.0529\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.8898 - acc: 0.0523\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 173.8721 - acc: 0.0516\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 173.8521 - acc: 0.0511\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - ETA: 0s - loss: 172.0932 - acc: 0.051 - 0s 5us/step - loss: 173.8351 - acc: 0.0516\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 173.8161 - acc: 0.0512\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 173.8006 - acc: 0.0513\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.7929 - acc: 0.0514\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.7742 - acc: 0.0519\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.7492 - acc: 0.0512\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.7311 - acc: 0.0518\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.7229 - acc: 0.0524\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.6972 - acc: 0.0515\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.6847 - acc: 0.0509\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 173.6614 - acc: 0.0512\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.6449 - acc: 0.0519\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.6327 - acc: 0.0523\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 173.6116 - acc: 0.0518\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.5929 - acc: 0.0513\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.5763 - acc: 0.0516\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.5573 - acc: 0.0518\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.5419 - acc: 0.0522\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.5252 - acc: 0.0518\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.5032 - acc: 0.0520\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.4884 - acc: 0.0519\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.4704 - acc: 0.0519\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.4571 - acc: 0.0521\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 173.4339 - acc: 0.0519\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 173.4324 - acc: 0.0516\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.4015 - acc: 0.0523\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.3843 - acc: 0.0516\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.3663 - acc: 0.0525\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.3530 - acc: 0.0520\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 173.3329 - acc: 0.0520\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.3109 - acc: 0.0513\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.3070 - acc: 0.0514\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 173.2795 - acc: 0.0514\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.2670 - acc: 0.0518\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.2428 - acc: 0.0524\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.2243 - acc: 0.0524\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.2040 - acc: 0.0519\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 173.1885 - acc: 0.0519\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.1804 - acc: 0.0520\n",
      "training set(after 1100 iterations) : 0.915\n",
      "23\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 173.1557 - acc: 0.0518\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.1442 - acc: 0.0521\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.1203 - acc: 0.0524\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.1012 - acc: 0.0515\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.0866 - acc: 0.0515\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 173.0767 - acc: 0.0520\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.0513 - acc: 0.0515\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.0458 - acc: 0.0514\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.0226 - acc: 0.0516\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 173.0065 - acc: 0.0516\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.9825 - acc: 0.0521\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.9668 - acc: 0.0525\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.9446 - acc: 0.0520\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 172.9299 - acc: 0.0518\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.9117 - acc: 0.0515\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.9083 - acc: 0.0529\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.8956 - acc: 0.0521\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.8560 - acc: 0.0513\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.8477 - acc: 0.0515\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.8245 - acc: 0.0515\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.8309 - acc: 0.0516\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.7917 - acc: 0.0518\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 172.7762 - acc: 0.0523\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.7554 - acc: 0.0519\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.7390 - acc: 0.0519\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.7214 - acc: 0.0523\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.7070 - acc: 0.0519\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.6925 - acc: 0.0524\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.6655 - acc: 0.0519\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.6469 - acc: 0.0521\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.6341 - acc: 0.0516\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.6168 - acc: 0.0516\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.5968 - acc: 0.0519\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.5788 - acc: 0.0522\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.5686 - acc: 0.0525\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.5512 - acc: 0.0519\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 172.5272 - acc: 0.0521\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.5137 - acc: 0.0518\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.5046 - acc: 0.0516\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.4798 - acc: 0.0520\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.4585 - acc: 0.0519\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.4420 - acc: 0.0521\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.4281 - acc: 0.0518\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 172.4102 - acc: 0.0515\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 172.3920 - acc: 0.0526\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 172.3744 - acc: 0.0523\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 172.3593 - acc: 0.0523\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.3501 - acc: 0.0523\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.3252 - acc: 0.0512\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 172.3168 - acc: 0.0520\n",
      "training set(after 1150 iterations) : 0.915\n",
      "24\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.2902 - acc: 0.0522\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.2784 - acc: 0.0525\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.2641 - acc: 0.0518\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.2372 - acc: 0.0516\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.2210 - acc: 0.0514\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.2039 - acc: 0.0518\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.1843 - acc: 0.0519\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.1651 - acc: 0.0519\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.1485 - acc: 0.0521\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.1332 - acc: 0.0522\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.1195 - acc: 0.0516\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.0955 - acc: 0.0515\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.0760 - acc: 0.0519\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 172.0634 - acc: 0.0524\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 172.0609 - acc: 0.0524\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.0269 - acc: 0.0515\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 172.0160 - acc: 0.0523\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.9920 - acc: 0.0528\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 171.9762 - acc: 0.0518\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.9583 - acc: 0.0516\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.9414 - acc: 0.0512\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.9283 - acc: 0.0519\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 171.9055 - acc: 0.0523\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.8885 - acc: 0.0518\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.8709 - acc: 0.0516\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.8514 - acc: 0.0520\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 171.8398 - acc: 0.0519\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.8175 - acc: 0.0518\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.8010 - acc: 0.0518\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.7914 - acc: 0.0519\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 171.7710 - acc: 0.0521\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.7553 - acc: 0.0512\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.7342 - acc: 0.0515\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.7188 - acc: 0.0515\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.6991 - acc: 0.0519\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.6864 - acc: 0.0512\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.6650 - acc: 0.0515\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.6443 - acc: 0.0515\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.6295 - acc: 0.0514\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.6075 - acc: 0.0514\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.5942 - acc: 0.0512\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.5800 - acc: 0.0511\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.5644 - acc: 0.0513\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.5369 - acc: 0.0518\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.5231 - acc: 0.0514\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.5053 - acc: 0.0515\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.4947 - acc: 0.0515\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.4754 - acc: 0.0510\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.4660 - acc: 0.0514\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.4394 - acc: 0.0515\n",
      "training set(after 1200 iterations) : 0.916\n",
      "25\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.4270 - acc: 0.0518\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 171.4084 - acc: 0.0526\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.3938 - acc: 0.0519\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 171.3676 - acc: 0.0519\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.3520 - acc: 0.0519\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.3349 - acc: 0.0515\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.3307 - acc: 0.0507\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.2958 - acc: 0.0511\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.2813 - acc: 0.0507\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.2721 - acc: 0.0520\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.2501 - acc: 0.0525\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.2368 - acc: 0.0515\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.2244 - acc: 0.0518\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.1961 - acc: 0.0518\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.1723 - acc: 0.0507\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.1665 - acc: 0.0523\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.1532 - acc: 0.0521\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.1246 - acc: 0.0511\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.1025 - acc: 0.0510\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.0884 - acc: 0.0512\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.0700 - acc: 0.0514\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 171.0521 - acc: 0.0518\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.0359 - acc: 0.0515\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.0185 - acc: 0.0509\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 171.0015 - acc: 0.0509\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.9879 - acc: 0.0510\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 8us/step - loss: 170.9774 - acc: 0.0511\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 170.9514 - acc: 0.0518\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 170.9274 - acc: 0.0514\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.9257 - acc: 0.0512\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.8979 - acc: 0.0515\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.8762 - acc: 0.0516\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.8660 - acc: 0.0512\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.8496 - acc: 0.0522\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.8272 - acc: 0.0515\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.8240 - acc: 0.0516\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.8005 - acc: 0.0509\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 170.7772 - acc: 0.0511\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.7613 - acc: 0.0513\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.7506 - acc: 0.0509\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 170.7483 - acc: 0.0512\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 170.7067 - acc: 0.0520\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.7019 - acc: 0.0516\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.6684 - acc: 0.0516\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.6659 - acc: 0.0512\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.6486 - acc: 0.0512\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.6202 - acc: 0.0515\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.6054 - acc: 0.0514\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.5876 - acc: 0.0509\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.5676 - acc: 0.0510\n",
      "training set(after 1250 iterations) : 0.916\n",
      "26\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.5590 - acc: 0.0515\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.5316 - acc: 0.0512\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.5101 - acc: 0.0514\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.4954 - acc: 0.0518\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.4793 - acc: 0.0519\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.4594 - acc: 0.0519\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.4436 - acc: 0.0515\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.4234 - acc: 0.0519\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.4102 - acc: 0.0516\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.3935 - acc: 0.0516\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 170.3690 - acc: 0.0513\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.3571 - acc: 0.0512\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.3437 - acc: 0.0513\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 170.3260 - acc: 0.0516\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.3062 - acc: 0.0520\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 170.2885 - acc: 0.0514\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 170.2756 - acc: 0.0515\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.2498 - acc: 0.0523\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 170.2294 - acc: 0.0520\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.2143 - acc: 0.0511\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.1978 - acc: 0.0518\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.1778 - acc: 0.0514\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.1598 - acc: 0.0515\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.1422 - acc: 0.0518\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.1316 - acc: 0.0522\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.1091 - acc: 0.0516\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.0928 - acc: 0.0515\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.0763 - acc: 0.0511\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.0551 - acc: 0.0514\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.0387 - acc: 0.0511\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 170.0210 - acc: 0.0512\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 170.0022 - acc: 0.0512\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.9950 - acc: 0.0516\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.9694 - acc: 0.0515\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.9492 - acc: 0.0522\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.9461 - acc: 0.0514\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.9194 - acc: 0.0518\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.8981 - acc: 0.0518\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 169.8836 - acc: 0.0518\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.8694 - acc: 0.0523\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 169.8472 - acc: 0.0523\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.8332 - acc: 0.0520\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.8129 - acc: 0.0518\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.7934 - acc: 0.0518\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.7739 - acc: 0.0511\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.7603 - acc: 0.0511\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.7415 - acc: 0.0512\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.7377 - acc: 0.0514\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.7126 - acc: 0.0515\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.7105 - acc: 0.0518\n",
      "training set(after 1300 iterations) : 0.917\n",
      "27\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.6814 - acc: 0.0523\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 169.6577 - acc: 0.0522\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.6350 - acc: 0.0515\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.6200 - acc: 0.0507\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.6014 - acc: 0.0511\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.5822 - acc: 0.0511\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.5669 - acc: 0.0524\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.5523 - acc: 0.0529\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.5328 - acc: 0.0523\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.5127 - acc: 0.0515\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 169.4992 - acc: 0.0513\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.4828 - acc: 0.0507\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.4590 - acc: 0.0509\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.4408 - acc: 0.0523\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.4483 - acc: 0.0522\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.4144 - acc: 0.0524\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.3901 - acc: 0.0522\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.3794 - acc: 0.0513\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.3614 - acc: 0.0507\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.3396 - acc: 0.0513\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.3230 - acc: 0.0523\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.3053 - acc: 0.0528\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.2921 - acc: 0.0529\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.2758 - acc: 0.0528\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 169.2663 - acc: 0.0522\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.2432 - acc: 0.0507\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 169.2273 - acc: 0.0505\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.2092 - acc: 0.0519\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.1878 - acc: 0.0526\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.1656 - acc: 0.0519\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 169.1497 - acc: 0.0526\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.1320 - acc: 0.0523\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 169.1221 - acc: 0.0524\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.1032 - acc: 0.0526\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.0844 - acc: 0.0532\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.0630 - acc: 0.0518\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.0524 - acc: 0.0507\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 169.0304 - acc: 0.0506\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 169.0138 - acc: 0.0519\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 169.0008 - acc: 0.0529\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.9858 - acc: 0.0531\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.9620 - acc: 0.0521\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.9425 - acc: 0.0515\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.9231 - acc: 0.0513\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.9369 - acc: 0.0519\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.8947 - acc: 0.0521\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.8745 - acc: 0.0513\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.8626 - acc: 0.0513\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.8397 - acc: 0.0522\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.8259 - acc: 0.0526\n",
      "training set(after 1350 iterations) : 0.917\n",
      "28\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 168.8158 - acc: 0.0531\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 168.7890 - acc: 0.0526\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 168.7681 - acc: 0.0510\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.7665 - acc: 0.0506\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.7488 - acc: 0.0510\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.7209 - acc: 0.0518\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.7035 - acc: 0.0525\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.6840 - acc: 0.0524\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.6706 - acc: 0.0519\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.6473 - acc: 0.0515\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.6368 - acc: 0.0524\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.6234 - acc: 0.0526\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.5941 - acc: 0.0522\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.5825 - acc: 0.0516\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.5643 - acc: 0.0520\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.5465 - acc: 0.0519\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.5320 - acc: 0.0520\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.5308 - acc: 0.0525\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.4953 - acc: 0.0528\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.4849 - acc: 0.0515\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.4614 - acc: 0.0513\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.4406 - acc: 0.0515\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 168.4436 - acc: 0.0522\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.4129 - acc: 0.0522\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.3896 - acc: 0.0520\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.3721 - acc: 0.0523\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.3611 - acc: 0.0521\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.3397 - acc: 0.0520\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.3225 - acc: 0.0522\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.3121 - acc: 0.0520\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.2951 - acc: 0.0520\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.2781 - acc: 0.0519\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.2541 - acc: 0.0522\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.2341 - acc: 0.0512\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.2227 - acc: 0.0514\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.2011 - acc: 0.0522\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.1844 - acc: 0.0520\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.1640 - acc: 0.0520\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.1484 - acc: 0.0518\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.1304 - acc: 0.0520\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.1128 - acc: 0.0514\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.0967 - acc: 0.0520\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.0863 - acc: 0.0519\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.0700 - acc: 0.0519\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 168.0457 - acc: 0.0520\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 168.0318 - acc: 0.0518\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 168.0114 - acc: 0.0516\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.9970 - acc: 0.0518\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.9727 - acc: 0.0518\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.9633 - acc: 0.0513\n",
      "training set(after 1400 iterations) : 0.918\n",
      "29\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.9421 - acc: 0.0519\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 167.9258 - acc: 0.0518\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.9148 - acc: 0.0515\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.8891 - acc: 0.0514\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.8752 - acc: 0.0512\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.8522 - acc: 0.0516\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.8370 - acc: 0.0516\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.8295 - acc: 0.0509\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.8134 - acc: 0.0519\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.7860 - acc: 0.0521\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 167.7736 - acc: 0.0524\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 167.7472 - acc: 0.0520\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 167.7354 - acc: 0.0516\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.7313 - acc: 0.0514\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 167.7104 - acc: 0.0504\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.6873 - acc: 0.0514\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.6678 - acc: 0.0516\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.6482 - acc: 0.0519\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 167.6337 - acc: 0.0516\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 167.6133 - acc: 0.0503\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.6041 - acc: 0.0504\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.5831 - acc: 0.0507\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.5718 - acc: 0.0513\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.5475 - acc: 0.0519\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 167.5273 - acc: 0.0519\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.5218 - acc: 0.0510\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.4932 - acc: 0.0506\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.4745 - acc: 0.0521\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.4626 - acc: 0.0516\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.4479 - acc: 0.0520\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.4227 - acc: 0.0518\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.4031 - acc: 0.0518\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.3941 - acc: 0.0500\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.3766 - acc: 0.0506\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.3551 - acc: 0.0510\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.3323 - acc: 0.0519\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.3270 - acc: 0.0515\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.3000 - acc: 0.0521\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.2787 - acc: 0.0519\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.2828 - acc: 0.0501\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.2540 - acc: 0.0501\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.2381 - acc: 0.0509\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.2244 - acc: 0.0522\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.2001 - acc: 0.0516\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.1758 - acc: 0.0521\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.1802 - acc: 0.0507\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.1546 - acc: 0.0506\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 167.1281 - acc: 0.0516\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.1191 - acc: 0.0518\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.1046 - acc: 0.0518\n",
      "training set(after 1450 iterations) : 0.918\n",
      "30\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.0767 - acc: 0.0520\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.0800 - acc: 0.0512\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.0455 - acc: 0.0514\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 167.0208 - acc: 0.0519\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 167.0144 - acc: 0.0518\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.9893 - acc: 0.0519\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 166.9683 - acc: 0.0513\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.9521 - acc: 0.0518\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.9357 - acc: 0.0510\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.9188 - acc: 0.0510\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.9033 - acc: 0.0518\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.8921 - acc: 0.0521\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.8705 - acc: 0.0516\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.8530 - acc: 0.0509\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.8343 - acc: 0.0512\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.8253 - acc: 0.0515\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.7967 - acc: 0.0515\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.7887 - acc: 0.0513\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.7648 - acc: 0.0515\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 166.7454 - acc: 0.0516\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.7268 - acc: 0.0515\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.7117 - acc: 0.0513\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 166.6976 - acc: 0.0518\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.6773 - acc: 0.0522\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.6764 - acc: 0.0516\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.6437 - acc: 0.0516\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.6245 - acc: 0.0521\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.6204 - acc: 0.0513\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.5998 - acc: 0.0519\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 166.5789 - acc: 0.0514\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 166.5632 - acc: 0.0507\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 166.5421 - acc: 0.0515\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 166.5223 - acc: 0.0519\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 166.5072 - acc: 0.0516\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.4894 - acc: 0.0521\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.4808 - acc: 0.0516\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.4511 - acc: 0.0519\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.4369 - acc: 0.0524\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.4163 - acc: 0.0516\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.4117 - acc: 0.0515\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.3896 - acc: 0.0515\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.3777 - acc: 0.0522\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.3494 - acc: 0.0515\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 166.3297 - acc: 0.0521\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.3184 - acc: 0.0521\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 166.3046 - acc: 0.0513\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.2844 - acc: 0.0521\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.2729 - acc: 0.0522\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.2453 - acc: 0.0515\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 166.2261 - acc: 0.0520\n",
      "training set(after 1500 iterations) : 0.919\n",
      "31\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.2116 - acc: 0.0516\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.1971 - acc: 0.0515\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.1765 - acc: 0.0519\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.1568 - acc: 0.0516\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.1412 - acc: 0.0518\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.1268 - acc: 0.0514\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.1108 - acc: 0.0516\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.0887 - acc: 0.0516\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.0743 - acc: 0.0522\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.0579 - acc: 0.0518\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.0391 - acc: 0.0518\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 166.0236 - acc: 0.0516\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 166.0060 - acc: 0.0524\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.9880 - acc: 0.0522\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.9680 - acc: 0.0514\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.9541 - acc: 0.0513\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 165.9344 - acc: 0.0520\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.9164 - acc: 0.0519\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.9048 - acc: 0.0519\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.8859 - acc: 0.0522\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.8898 - acc: 0.0512\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.8549 - acc: 0.0509\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.8332 - acc: 0.0515\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.8222 - acc: 0.0522\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.8125 - acc: 0.0513\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.7818 - acc: 0.0510\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.7653 - acc: 0.0515\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 165.7581 - acc: 0.0518\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.7324 - acc: 0.0518\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.7161 - acc: 0.0521\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 165.6971 - acc: 0.0521\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.6783 - acc: 0.0518\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.6632 - acc: 0.0518\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.6479 - acc: 0.0516\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.6280 - acc: 0.0512\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.6122 - acc: 0.0511\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 165.5936 - acc: 0.0515\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.5791 - acc: 0.0518\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.5665 - acc: 0.0519\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.5458 - acc: 0.0519\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.5255 - acc: 0.0520\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.5172 - acc: 0.0518\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.4930 - acc: 0.0518\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.4756 - acc: 0.0519\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.4597 - acc: 0.0521\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.4506 - acc: 0.0512\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 165.4289 - acc: 0.0513\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.4064 - acc: 0.0520\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.3926 - acc: 0.0513\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.3756 - acc: 0.0513\n",
      "training set(after 1550 iterations) : 0.919\n",
      "32\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.3586 - acc: 0.0512\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.3458 - acc: 0.0511\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.3268 - acc: 0.0518\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.3078 - acc: 0.0510\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.2895 - acc: 0.0518\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.2829 - acc: 0.0520\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.2596 - acc: 0.0514\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.2432 - acc: 0.0514\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 165.2246 - acc: 0.0514\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.2079 - acc: 0.0514\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 165.1892 - acc: 0.0512\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.1742 - acc: 0.0516\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.1568 - acc: 0.0518\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 165.1392 - acc: 0.0518\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.1244 - acc: 0.0520\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.1209 - acc: 0.0519\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 165.0946 - acc: 0.0513\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.0719 - acc: 0.0516\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.0637 - acc: 0.0511\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 2us/step - loss: 165.0385 - acc: 0.0513\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 165.0303 - acc: 0.0511\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 165.0140 - acc: 0.0521\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.9943 - acc: 0.0512\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 164.9709 - acc: 0.0515\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.9725 - acc: 0.0516\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.9359 - acc: 0.0518\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.9189 - acc: 0.0516\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.9058 - acc: 0.0515\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.8856 - acc: 0.0510\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.8719 - acc: 0.0510\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.8520 - acc: 0.0513\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.8404 - acc: 0.0511\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.8275 - acc: 0.0514\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.8072 - acc: 0.0514\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.7877 - acc: 0.0518\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.7673 - acc: 0.0512\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.7540 - acc: 0.0518\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 164.7388 - acc: 0.0522\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.7250 - acc: 0.0516\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 164.6998 - acc: 0.0515\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.6851 - acc: 0.0521\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.6720 - acc: 0.0520\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.6564 - acc: 0.0513\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 164.6353 - acc: 0.0515\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.6235 - acc: 0.0522\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.6003 - acc: 0.0519\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.5859 - acc: 0.0519\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.5704 - acc: 0.0519\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.5526 - acc: 0.0516\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.5498 - acc: 0.0528\n",
      "training set(after 1600 iterations) : 0.919\n",
      "33\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 164.5215 - acc: 0.0524\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.5010 - acc: 0.0514\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.4892 - acc: 0.0518\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.4773 - acc: 0.0518\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.4523 - acc: 0.0522\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.4352 - acc: 0.0523\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.4183 - acc: 0.0519\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.4033 - acc: 0.0519\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.3903 - acc: 0.0515\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.3691 - acc: 0.0512\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.3517 - acc: 0.0519\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.3344 - acc: 0.0518\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.3171 - acc: 0.0516\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.3078 - acc: 0.0515\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.2871 - acc: 0.0512\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 164.2709 - acc: 0.0519\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.2536 - acc: 0.0514\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 164.2358 - acc: 0.0521\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.2303 - acc: 0.0521\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.2020 - acc: 0.0523\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.1867 - acc: 0.0520\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.1699 - acc: 0.0512\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.1555 - acc: 0.0518\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.1354 - acc: 0.0521\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.1215 - acc: 0.0521\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.1147 - acc: 0.0520\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.0873 - acc: 0.0518\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.0830 - acc: 0.0522\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 164.0544 - acc: 0.0516\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.0389 - acc: 0.0525\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.0295 - acc: 0.0525\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 164.0048 - acc: 0.0516\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.9862 - acc: 0.0513\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.9667 - acc: 0.0516\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.9660 - acc: 0.0513\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.9574 - acc: 0.0513\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.9226 - acc: 0.0528\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.9036 - acc: 0.0519\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.8889 - acc: 0.0520\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.8745 - acc: 0.0519\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.8550 - acc: 0.0519\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.8749 - acc: 0.0516\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.8341 - acc: 0.0523\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.8307 - acc: 0.0519\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.7990 - acc: 0.0516\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.7832 - acc: 0.0515\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.7590 - acc: 0.0521\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.7408 - acc: 0.0514\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.7199 - acc: 0.0512\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.7112 - acc: 0.0520\n",
      "training set(after 1650 iterations) : 0.920\n",
      "34\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.6930 - acc: 0.0515\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.6709 - acc: 0.0514\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.6553 - acc: 0.0520\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.6433 - acc: 0.0521\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.6260 - acc: 0.0520\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.6054 - acc: 0.0521\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.5894 - acc: 0.0523\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.5706 - acc: 0.0522\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.5534 - acc: 0.0516\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.5361 - acc: 0.0523\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.5222 - acc: 0.0521\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.5064 - acc: 0.0521\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.4895 - acc: 0.0522\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.4788 - acc: 0.0522\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.4533 - acc: 0.0523\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.4389 - acc: 0.0522\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.4229 - acc: 0.0524\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.4090 - acc: 0.0521\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.3942 - acc: 0.0524\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.3769 - acc: 0.0526\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.3535 - acc: 0.0525\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.3387 - acc: 0.0525\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.3266 - acc: 0.0518\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.3092 - acc: 0.0525\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.3102 - acc: 0.0524\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.2701 - acc: 0.0522\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.2637 - acc: 0.0526\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.2550 - acc: 0.0529\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.2293 - acc: 0.0524\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.2091 - acc: 0.0519\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.1942 - acc: 0.0523\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.1789 - acc: 0.0528\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.1737 - acc: 0.0516\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.1449 - acc: 0.0523\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 163.1252 - acc: 0.0529\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.1165 - acc: 0.0522\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.1017 - acc: 0.0521\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.0790 - acc: 0.0525\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.0679 - acc: 0.0522\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 163.0451 - acc: 0.0525\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 163.0362 - acc: 0.0526\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 163.0153 - acc: 0.0523\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.9955 - acc: 0.0525\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.9882 - acc: 0.0532\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.9650 - acc: 0.0525\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.9524 - acc: 0.0526\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.9294 - acc: 0.0523\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.9162 - acc: 0.0525\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.9048 - acc: 0.0525\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.8844 - acc: 0.0525\n",
      "training set(after 1700 iterations) : 0.920\n",
      "35\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.8682 - acc: 0.0529\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.8489 - acc: 0.0522\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.8342 - acc: 0.0525\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.8174 - acc: 0.0524\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.7983 - acc: 0.0523\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.7904 - acc: 0.0529\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 162.7714 - acc: 0.0530\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 162.7477 - acc: 0.0520\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.7375 - acc: 0.0522\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.7348 - acc: 0.0526\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.7070 - acc: 0.0518\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.6867 - acc: 0.0525\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.6731 - acc: 0.0525\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.6625 - acc: 0.0531\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.6420 - acc: 0.0523\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.6330 - acc: 0.0522\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.6067 - acc: 0.0526\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.5898 - acc: 0.0528\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.5762 - acc: 0.0528\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.5585 - acc: 0.0528\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.5412 - acc: 0.0525\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.5225 - acc: 0.0522\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.5067 - acc: 0.0526\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.4923 - acc: 0.0529\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 162.4762 - acc: 0.0529\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.4563 - acc: 0.0526\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.4520 - acc: 0.0534\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.4368 - acc: 0.0526\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.4103 - acc: 0.0524\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.3945 - acc: 0.0522\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.3856 - acc: 0.0522\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.3609 - acc: 0.0534\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.3432 - acc: 0.0531\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 162.3289 - acc: 0.0528\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 162.3133 - acc: 0.0526\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 162.3002 - acc: 0.0530\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.2798 - acc: 0.0532\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 162.2653 - acc: 0.0534\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.2443 - acc: 0.0533\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.2287 - acc: 0.0521\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 162.2296 - acc: 0.0528\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.2011 - acc: 0.0525\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 162.1809 - acc: 0.0525\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 162.1671 - acc: 0.0535\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 162.1528 - acc: 0.0533\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 162.1315 - acc: 0.0534\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 162.1159 - acc: 0.0534\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.1009 - acc: 0.0531\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 162.0832 - acc: 0.0531\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 162.0659 - acc: 0.0533\n",
      "training set(after 1750 iterations) : 0.921\n",
      "36\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.0492 - acc: 0.0537\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.0377 - acc: 0.0538\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.0243 - acc: 0.0535\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.0020 - acc: 0.0533\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 162.0053 - acc: 0.0526\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.9702 - acc: 0.0529\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.9654 - acc: 0.0534\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.9463 - acc: 0.0534\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.9251 - acc: 0.0538\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.9062 - acc: 0.0524\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.8913 - acc: 0.0530\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.8836 - acc: 0.0526\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.8555 - acc: 0.0529\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.8588 - acc: 0.0529\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.8520 - acc: 0.0530\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.8151 - acc: 0.0532\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.7999 - acc: 0.0537\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.7893 - acc: 0.0531\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.7627 - acc: 0.0532\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.7574 - acc: 0.0533\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.7386 - acc: 0.0530\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.7155 - acc: 0.0526\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.6983 - acc: 0.0531\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.6825 - acc: 0.0525\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.6681 - acc: 0.0525\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.6501 - acc: 0.0526\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.6322 - acc: 0.0525\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.6418 - acc: 0.0523\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.6157 - acc: 0.0531\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.5945 - acc: 0.0531\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.5831 - acc: 0.0538\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.5655 - acc: 0.0532\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.5509 - acc: 0.0532\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.5286 - acc: 0.0538\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.5103 - acc: 0.0534\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.4934 - acc: 0.0531\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.4724 - acc: 0.0533\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.4598 - acc: 0.0529\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.4502 - acc: 0.0531\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.4269 - acc: 0.0531\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.4079 - acc: 0.0531\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.4006 - acc: 0.0535\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.3825 - acc: 0.0538\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.3645 - acc: 0.0537\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.3753 - acc: 0.0541\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.3704 - acc: 0.0535\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 161.3270 - acc: 0.0537\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.3047 - acc: 0.0530\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.2888 - acc: 0.0538\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 161.2706 - acc: 0.0537\n",
      "training set(after 1800 iterations) : 0.921\n",
      "37\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 161.2534 - acc: 0.0538\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 161.2292 - acc: 0.0533\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.2166 - acc: 0.0540\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.2013 - acc: 0.0538\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.1834 - acc: 0.0530\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.1759 - acc: 0.0538\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.1544 - acc: 0.0538\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.1335 - acc: 0.0534\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 161.1248 - acc: 0.0540\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.1111 - acc: 0.0538\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 161.0932 - acc: 0.0540\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.0783 - acc: 0.0538\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.0673 - acc: 0.0544\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.0427 - acc: 0.0539\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.0233 - acc: 0.0531\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.0119 - acc: 0.0538\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 161.0111 - acc: 0.0533\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.9794 - acc: 0.0541\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 160.9734 - acc: 0.0541\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.9536 - acc: 0.0530\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.9497 - acc: 0.0532\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.9100 - acc: 0.0544\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.8990 - acc: 0.0543\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.9079 - acc: 0.0540\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.8773 - acc: 0.0540\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.8450 - acc: 0.0541\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.8422 - acc: 0.0533\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.8352 - acc: 0.0529\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.8054 - acc: 0.0524\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.7861 - acc: 0.0538\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.7722 - acc: 0.0546\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.7605 - acc: 0.0544\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.7403 - acc: 0.0540\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.7227 - acc: 0.0539\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.7108 - acc: 0.0534\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.6960 - acc: 0.0532\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.6775 - acc: 0.0530\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.6803 - acc: 0.0541\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.6514 - acc: 0.0546\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.6333 - acc: 0.0541\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.6128 - acc: 0.0537\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.6000 - acc: 0.0538\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.5831 - acc: 0.0539\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 160.5692 - acc: 0.0540\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.5516 - acc: 0.0541\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 160.5410 - acc: 0.0538\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.5303 - acc: 0.0539\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.5073 - acc: 0.0538\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.4899 - acc: 0.0535\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.4718 - acc: 0.0541\n",
      "training set(after 1850 iterations) : 0.921\n",
      "38\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.4572 - acc: 0.0540\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.4450 - acc: 0.0538\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.4306 - acc: 0.0538\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.4113 - acc: 0.0533\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.3936 - acc: 0.0535\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.3854 - acc: 0.0543\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.3742 - acc: 0.0543\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.3601 - acc: 0.0539\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.3412 - acc: 0.0532\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 160.3203 - acc: 0.0535\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.3091 - acc: 0.0532\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 160.2896 - acc: 0.0534\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 160.2785 - acc: 0.0542\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 160.2611 - acc: 0.0537\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.2416 - acc: 0.0540\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.2271 - acc: 0.0534\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.2178 - acc: 0.0531\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.1982 - acc: 0.0529\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.1764 - acc: 0.0533\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.1636 - acc: 0.0541\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.1512 - acc: 0.0537\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 160.1332 - acc: 0.0541\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.1194 - acc: 0.0542\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.0984 - acc: 0.0539\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.0906 - acc: 0.0535\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 160.0708 - acc: 0.0537\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.0785 - acc: 0.0539\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.0577 - acc: 0.0538\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 160.0237 - acc: 0.0535\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 160.0092 - acc: 0.0531\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.9888 - acc: 0.0533\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.9775 - acc: 0.0533\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.9662 - acc: 0.0532\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.9489 - acc: 0.0534\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.9282 - acc: 0.0538\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.9296 - acc: 0.0541\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.8934 - acc: 0.0534\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.8865 - acc: 0.0533\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.8958 - acc: 0.0530\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.8482 - acc: 0.0534\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.8350 - acc: 0.0535\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.8198 - acc: 0.0531\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.8076 - acc: 0.0539\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.7891 - acc: 0.0538\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.7733 - acc: 0.0539\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.7585 - acc: 0.0538\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 159.7449 - acc: 0.0538\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.7296 - acc: 0.0534\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.7083 - acc: 0.0530\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.6896 - acc: 0.0539\n",
      "training set(after 1900 iterations) : 0.922\n",
      "39\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.6763 - acc: 0.0532\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.6643 - acc: 0.0535\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.6582 - acc: 0.0539\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.6422 - acc: 0.0537\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.6228 - acc: 0.0539\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.5993 - acc: 0.0535\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 159.5794 - acc: 0.0533\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 159.5721 - acc: 0.0530\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.5525 - acc: 0.0534\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.5433 - acc: 0.0537\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 159.5323 - acc: 0.0538\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.5053 - acc: 0.0534\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.4896 - acc: 0.0534\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.4726 - acc: 0.0538\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.4569 - acc: 0.0539\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.4473 - acc: 0.0531\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.4275 - acc: 0.0539\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.4145 - acc: 0.0535\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.3963 - acc: 0.0534\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.3785 - acc: 0.0541\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 159.3749 - acc: 0.0532\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.3447 - acc: 0.0531\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.3324 - acc: 0.0533\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.3207 - acc: 0.0529\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.3021 - acc: 0.0533\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 159.2839 - acc: 0.0535\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 159.2829 - acc: 0.0540\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.2510 - acc: 0.0539\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 159.2446 - acc: 0.0533\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.2299 - acc: 0.0534\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.2134 - acc: 0.0537\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.1868 - acc: 0.0539\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 159.1858 - acc: 0.0535\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.1711 - acc: 0.0538\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.1410 - acc: 0.0537\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.1472 - acc: 0.0533\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.1103 - acc: 0.0539\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.0993 - acc: 0.0541\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.0806 - acc: 0.0543\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 159.0667 - acc: 0.0535\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.0471 - acc: 0.0534\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 159.0325 - acc: 0.0533\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.0358 - acc: 0.0537\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 159.0103 - acc: 0.0537\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 158.9875 - acc: 0.0538\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.9672 - acc: 0.0535\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.9528 - acc: 0.0540\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.9373 - acc: 0.0539\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.9314 - acc: 0.0537\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.9044 - acc: 0.0539\n",
      "training set(after 1950 iterations) : 0.922\n",
      "40\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 158.8892 - acc: 0.0541\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.8705 - acc: 0.0539\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.8634 - acc: 0.0544\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.8429 - acc: 0.0544\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.8250 - acc: 0.0534\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.8075 - acc: 0.0534\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.7990 - acc: 0.0531\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.7783 - acc: 0.0530\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 158.7562 - acc: 0.0530\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.7496 - acc: 0.0544\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.7405 - acc: 0.0539\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 158.7121 - acc: 0.0537\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 158.6955 - acc: 0.0532\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.6796 - acc: 0.0532\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.6649 - acc: 0.0535\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.6582 - acc: 0.0544\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 158.6334 - acc: 0.0543\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.6211 - acc: 0.0542\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.5967 - acc: 0.0542\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.5891 - acc: 0.0528\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.5768 - acc: 0.0532\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.5484 - acc: 0.0539\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.5469 - acc: 0.0531\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.5315 - acc: 0.0544\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.5082 - acc: 0.0539\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.5016 - acc: 0.0533\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.4845 - acc: 0.0535\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.4584 - acc: 0.0533\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.4478 - acc: 0.0540\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.4337 - acc: 0.0535\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.4130 - acc: 0.0537\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.3975 - acc: 0.0534\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.3815 - acc: 0.0537\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.3615 - acc: 0.0531\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.3461 - acc: 0.0533\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.3341 - acc: 0.0538\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 158.3170 - acc: 0.0540\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 158.3213 - acc: 0.0535\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.2875 - acc: 0.0534\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.2660 - acc: 0.0541\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 158.2543 - acc: 0.0538\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.2472 - acc: 0.0538\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.2231 - acc: 0.0537\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.2052 - acc: 0.0537\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.1887 - acc: 0.0539\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.1693 - acc: 0.0530\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.1578 - acc: 0.0541\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.1500 - acc: 0.0542\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.1323 - acc: 0.0537\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.1087 - acc: 0.0539\n",
      "training set(after 2000 iterations) : 0.923\n",
      "41\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 158.0936 - acc: 0.0533\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.0755 - acc: 0.0535\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.0623 - acc: 0.0533\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.0461 - acc: 0.0537\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.0447 - acc: 0.0542\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.0188 - acc: 0.0539\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.9990 - acc: 0.0541\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 158.0008 - acc: 0.0538\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.9794 - acc: 0.0543\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.9583 - acc: 0.0543\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.9417 - acc: 0.0540\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.9279 - acc: 0.0538\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 157.9243 - acc: 0.0540\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.8897 - acc: 0.0542\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.8769 - acc: 0.0533\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 157.8628 - acc: 0.0539\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.8399 - acc: 0.0537\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.8267 - acc: 0.0542\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.8222 - acc: 0.0540\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 157.7970 - acc: 0.0540\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.7791 - acc: 0.0542\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.7674 - acc: 0.0541\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.7476 - acc: 0.0540\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 157.7381 - acc: 0.0541\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.7216 - acc: 0.0538\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.6995 - acc: 0.0540\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.6896 - acc: 0.0537\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.6728 - acc: 0.0535\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 157.6572 - acc: 0.0541\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 157.6395 - acc: 0.0546\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 157.6269 - acc: 0.0542\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.6112 - acc: 0.0533\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.5931 - acc: 0.0537\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.5852 - acc: 0.0542\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.5613 - acc: 0.0542\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.5613 - acc: 0.0538\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.5263 - acc: 0.0549\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.5138 - acc: 0.0532\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.5006 - acc: 0.0540\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.4882 - acc: 0.0540\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.4794 - acc: 0.0539\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.4501 - acc: 0.0541\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.4325 - acc: 0.0538\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 157.4182 - acc: 0.0540\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.4024 - acc: 0.0538\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 157.4097 - acc: 0.0542\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.3722 - acc: 0.0543\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.3778 - acc: 0.0538\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.3433 - acc: 0.0538\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.3209 - acc: 0.0543\n",
      "training set(after 2050 iterations) : 0.923\n",
      "42\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.3203 - acc: 0.0551\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.3111 - acc: 0.0541\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 157.2777 - acc: 0.0540\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.2604 - acc: 0.0541\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.2573 - acc: 0.0546\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.2326 - acc: 0.0540\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.2221 - acc: 0.0547\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.2015 - acc: 0.0542\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.1847 - acc: 0.0535\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.1690 - acc: 0.0538\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.1571 - acc: 0.0542\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.1507 - acc: 0.0534\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.1279 - acc: 0.0540\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.1100 - acc: 0.0541\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.0992 - acc: 0.0541\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.0760 - acc: 0.0540\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 157.0654 - acc: 0.0543\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.0521 - acc: 0.0541\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 157.0294 - acc: 0.0538\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.0235 - acc: 0.0538\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 157.0108 - acc: 0.0543\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.9949 - acc: 0.0538\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.9741 - acc: 0.0543\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.9570 - acc: 0.0541\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.9492 - acc: 0.0541\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.9282 - acc: 0.0540\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.9075 - acc: 0.0541\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.9030 - acc: 0.0541\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.8814 - acc: 0.0542\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.8664 - acc: 0.0549\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 156.8472 - acc: 0.0548\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 156.8299 - acc: 0.0548\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.8536 - acc: 0.0546\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.8524 - acc: 0.0546\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.7868 - acc: 0.0546\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.7699 - acc: 0.0543\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.7661 - acc: 0.0544\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.7425 - acc: 0.0541\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.7249 - acc: 0.0548\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.7108 - acc: 0.0541\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.7025 - acc: 0.0539\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 156.6849 - acc: 0.0543\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.6650 - acc: 0.0551\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.6620 - acc: 0.0552\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.6331 - acc: 0.0549\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.6320 - acc: 0.0552\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.6126 - acc: 0.0546\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.5992 - acc: 0.0549\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.5702 - acc: 0.0548\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.5556 - acc: 0.0550\n",
      "training set(after 2100 iterations) : 0.923\n",
      "43\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.5407 - acc: 0.0547\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.5302 - acc: 0.0549\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.5103 - acc: 0.0552\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.4968 - acc: 0.0549\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.4871 - acc: 0.0551\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.4653 - acc: 0.0544\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.4487 - acc: 0.0548\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.4441 - acc: 0.0547\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.4233 - acc: 0.0552\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 156.4071 - acc: 0.0553\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 156.3901 - acc: 0.0553\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.3883 - acc: 0.0550\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.3606 - acc: 0.0557\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.3497 - acc: 0.0548\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.3422 - acc: 0.0547\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.3280 - acc: 0.0539\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.3010 - acc: 0.0555\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.2916 - acc: 0.0547\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.2883 - acc: 0.0544\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.2518 - acc: 0.0541\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.2352 - acc: 0.0549\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 156.2269 - acc: 0.0550\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 156.2026 - acc: 0.0543\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 156.1887 - acc: 0.0557\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.1806 - acc: 0.0547\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.1635 - acc: 0.0555\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 156.1435 - acc: 0.0549\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.1297 - acc: 0.0551\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.1160 - acc: 0.0552\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.1013 - acc: 0.0550\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 156.1015 - acc: 0.0558\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.0713 - acc: 0.0556\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 156.0537 - acc: 0.0555\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.0395 - acc: 0.0550\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.0258 - acc: 0.0557\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 156.0064 - acc: 0.0555\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.9894 - acc: 0.0553\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.9819 - acc: 0.0555\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.9607 - acc: 0.0551\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.9484 - acc: 0.0555\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.9258 - acc: 0.0553\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.9264 - acc: 0.0559\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.9018 - acc: 0.0560\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.8894 - acc: 0.0555\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.8695 - acc: 0.0552\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.8497 - acc: 0.0556\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.8337 - acc: 0.0553\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.8210 - acc: 0.0555\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.8176 - acc: 0.0551\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.7915 - acc: 0.0557\n",
      "training set(after 2150 iterations) : 0.924\n",
      "44\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.7759 - acc: 0.0553\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.7689 - acc: 0.0551\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.7489 - acc: 0.0557\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.7415 - acc: 0.0559\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.7190 - acc: 0.0551\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.6977 - acc: 0.0557\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.6867 - acc: 0.0557\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.6841 - acc: 0.0564\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.6564 - acc: 0.0558\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.6416 - acc: 0.0560\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.6317 - acc: 0.0555\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.6141 - acc: 0.0549\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.5912 - acc: 0.0557\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.5839 - acc: 0.0557\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.5753 - acc: 0.0557\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 155.5536 - acc: 0.0558\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.5387 - acc: 0.0555\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.5248 - acc: 0.0549\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.5089 - acc: 0.0550\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.4917 - acc: 0.0555\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.4772 - acc: 0.0564\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.4664 - acc: 0.0564\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.4456 - acc: 0.0559\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.4423 - acc: 0.0561\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.4126 - acc: 0.0557\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.4105 - acc: 0.0547\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.3915 - acc: 0.0549\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.3708 - acc: 0.0556\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.3584 - acc: 0.0558\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.3420 - acc: 0.0557\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.3270 - acc: 0.0566\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.3075 - acc: 0.0560\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.2970 - acc: 0.0557\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.2899 - acc: 0.0557\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.2609 - acc: 0.0558\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.2520 - acc: 0.0560\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.2379 - acc: 0.0560\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.2193 - acc: 0.0557\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.2056 - acc: 0.0555\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.1900 - acc: 0.0555\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 155.1744 - acc: 0.0562\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.1714 - acc: 0.0562\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.1440 - acc: 0.0556\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.1361 - acc: 0.0558\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.1104 - acc: 0.0562\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.1193 - acc: 0.0573\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.1187 - acc: 0.0565\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.0660 - acc: 0.0558\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 155.0554 - acc: 0.0560\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.0449 - acc: 0.0558\n",
      "training set(after 2200 iterations) : 0.924\n",
      "45\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.0223 - acc: 0.0560\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 155.0105 - acc: 0.0564\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.9934 - acc: 0.0558\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.9795 - acc: 0.0559\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.9625 - acc: 0.0566\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.9563 - acc: 0.0558\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.9353 - acc: 0.0558\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.9223 - acc: 0.0552\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.9128 - acc: 0.0557\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.8936 - acc: 0.0552\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.9029 - acc: 0.0566\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.8667 - acc: 0.0566\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.8489 - acc: 0.0561\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.8338 - acc: 0.0556\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.8226 - acc: 0.0550\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.8037 - acc: 0.0551\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.7822 - acc: 0.0552\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.7699 - acc: 0.0559\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.7560 - acc: 0.0558\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.7576 - acc: 0.0565\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 154.7369 - acc: 0.0561\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.7154 - acc: 0.0553\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.7041 - acc: 0.0558\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.6783 - acc: 0.0555\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.6645 - acc: 0.0558\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.6624 - acc: 0.0553\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.6472 - acc: 0.0566\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.6263 - acc: 0.0571\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.6054 - acc: 0.0555\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.6118 - acc: 0.0551\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.6037 - acc: 0.0558\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.5643 - acc: 0.0551\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.5484 - acc: 0.0557\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.5332 - acc: 0.0557\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.5196 - acc: 0.0559\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.5066 - acc: 0.0558\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.4883 - acc: 0.0562\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.4723 - acc: 0.0557\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.4690 - acc: 0.0558\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.4490 - acc: 0.0556\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.4292 - acc: 0.0555\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.4149 - acc: 0.0561\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.4008 - acc: 0.0566\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.3886 - acc: 0.0561\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.3767 - acc: 0.0552\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 154.3585 - acc: 0.0558\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.3395 - acc: 0.0559\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.3297 - acc: 0.0564\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.3132 - acc: 0.0568\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.2947 - acc: 0.0562\n",
      "training set(after 2250 iterations) : 0.925\n",
      "46\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.2821 - acc: 0.0558\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.2658 - acc: 0.0558\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 154.2639 - acc: 0.0557\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.2381 - acc: 0.0562\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.2249 - acc: 0.0561\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.2119 - acc: 0.0566\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.1923 - acc: 0.0566\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.1769 - acc: 0.0560\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.1682 - acc: 0.0562\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.1498 - acc: 0.0566\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.1328 - acc: 0.0557\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.1177 - acc: 0.0561\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.1043 - acc: 0.0565\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.0884 - acc: 0.0564\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.0743 - acc: 0.0562\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.0684 - acc: 0.0571\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.0461 - acc: 0.0567\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 154.0318 - acc: 0.0560\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 154.0198 - acc: 0.0559\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 154.0062 - acc: 0.0561\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.9861 - acc: 0.0561\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.9869 - acc: 0.0568\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.9617 - acc: 0.0566\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.9484 - acc: 0.0584\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.9321 - acc: 0.0573\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.9175 - acc: 0.0565\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.9059 - acc: 0.0565\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.8826 - acc: 0.0566\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.8771 - acc: 0.0565\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.8528 - acc: 0.0561\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.8389 - acc: 0.0559\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.8286 - acc: 0.0559\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.8172 - acc: 0.0560\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.8056 - acc: 0.0570\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.7790 - acc: 0.0568\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.7920 - acc: 0.0579\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.7576 - acc: 0.0579\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.7367 - acc: 0.0568\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.7356 - acc: 0.0565\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.7075 - acc: 0.0569\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.7094 - acc: 0.0581\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.6832 - acc: 0.0585\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.6608 - acc: 0.0581\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.6487 - acc: 0.0571\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.6342 - acc: 0.0565\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.6250 - acc: 0.0564\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.6198 - acc: 0.0564\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.5878 - acc: 0.0579\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.5882 - acc: 0.0587\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.5722 - acc: 0.0590\n",
      "training set(after 2300 iterations) : 0.925\n",
      "47\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 153.5450 - acc: 0.0593\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.5296 - acc: 0.0566\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.5284 - acc: 0.0569\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.5123 - acc: 0.0568\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.4923 - acc: 0.0568\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.4708 - acc: 0.0573\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.4714 - acc: 0.0587\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.4437 - acc: 0.0588\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.4414 - acc: 0.0588\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.4198 - acc: 0.0592\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.4017 - acc: 0.0571\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.3870 - acc: 0.0562\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.3776 - acc: 0.0566\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.3585 - acc: 0.0566\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.3413 - acc: 0.0578\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.3345 - acc: 0.0577\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.3101 - acc: 0.0584\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.2974 - acc: 0.0597\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.2900 - acc: 0.0589\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.2682 - acc: 0.0568\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.2570 - acc: 0.0576\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.2354 - acc: 0.0587\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.2232 - acc: 0.0589\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.2139 - acc: 0.0595\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.1906 - acc: 0.0586\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.1791 - acc: 0.0568\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.1786 - acc: 0.0567\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.1506 - acc: 0.0565\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.1455 - acc: 0.0585\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.1281 - acc: 0.0599\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.1122 - acc: 0.0597\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.0879 - acc: 0.0575\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.0797 - acc: 0.0573\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.0627 - acc: 0.0569\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.0530 - acc: 0.0583\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 153.0446 - acc: 0.0598\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 153.0223 - acc: 0.0593\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 153.0007 - acc: 0.0581\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.9877 - acc: 0.0573\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.9887 - acc: 0.0575\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 152.9547 - acc: 0.0571\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.9717 - acc: 0.0593\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.9415 - acc: 0.0599\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.9225 - acc: 0.0592\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.9085 - acc: 0.0576\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.8908 - acc: 0.0569\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 152.8713 - acc: 0.0567\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.8566 - acc: 0.0568\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 152.8423 - acc: 0.0585\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 152.8512 - acc: 0.0595\n",
      "training set(after 2350 iterations) : 0.925\n",
      "48\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 152.8144 - acc: 0.0599\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.7993 - acc: 0.0578\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.7868 - acc: 0.0574\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.7743 - acc: 0.0574\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.7891 - acc: 0.0587\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.7427 - acc: 0.0602\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.7287 - acc: 0.0599\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.7076 - acc: 0.0589\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.6954 - acc: 0.0586\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.6807 - acc: 0.0586\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.6658 - acc: 0.0588\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 152.6477 - acc: 0.0587\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 152.6388 - acc: 0.0580\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.6191 - acc: 0.0583\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.6366 - acc: 0.0597\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 152.5964 - acc: 0.0593\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.5797 - acc: 0.0593\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.5721 - acc: 0.0580\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.5624 - acc: 0.0587\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 152.5353 - acc: 0.0595\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.5274 - acc: 0.0598\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.5122 - acc: 0.0588\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.4930 - acc: 0.0594\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 152.4821 - acc: 0.0593\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.4776 - acc: 0.0583\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.4436 - acc: 0.0589\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.4395 - acc: 0.0598\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.4215 - acc: 0.0607\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.4064 - acc: 0.0596\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.4143 - acc: 0.0592\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 152.3794 - acc: 0.0583\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.3747 - acc: 0.0603\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.3452 - acc: 0.0606\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.3410 - acc: 0.0593\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.3164 - acc: 0.0588\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 152.3003 - acc: 0.0592\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.2882 - acc: 0.0599\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.2752 - acc: 0.0602\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.2573 - acc: 0.0599\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 152.2425 - acc: 0.0593\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 7us/step - loss: 152.2354 - acc: 0.0584\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.2161 - acc: 0.0589\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 152.2010 - acc: 0.0604\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.1877 - acc: 0.0605\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.1718 - acc: 0.0603\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.1539 - acc: 0.0597\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.1478 - acc: 0.0589\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.1302 - acc: 0.0587\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.1166 - acc: 0.0602\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.0985 - acc: 0.0604\n",
      "training set(after 2400 iterations) : 0.926\n",
      "49\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 152.0878 - acc: 0.0605\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.0696 - acc: 0.0606\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.0581 - acc: 0.0601\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.0394 - acc: 0.0597\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.0454 - acc: 0.0595\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 152.0141 - acc: 0.0587\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 152.0211 - acc: 0.0601\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.9905 - acc: 0.0606\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.9651 - acc: 0.0608\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.9740 - acc: 0.0592\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.9479 - acc: 0.0581\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.9240 - acc: 0.0597\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.9136 - acc: 0.0599\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.9034 - acc: 0.0603\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.8953 - acc: 0.0605\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.8666 - acc: 0.0597\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.8709 - acc: 0.0581\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.8510 - acc: 0.0583\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.8227 - acc: 0.0596\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.8264 - acc: 0.0602\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.8210 - acc: 0.0597\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.7906 - acc: 0.0594\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.7654 - acc: 0.0596\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.7567 - acc: 0.0598\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.7418 - acc: 0.0601\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.7236 - acc: 0.0599\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.7084 - acc: 0.0598\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.7030 - acc: 0.0595\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.6823 - acc: 0.0599\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.6704 - acc: 0.0602\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.6552 - acc: 0.0596\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.6372 - acc: 0.0599\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.6237 - acc: 0.0597\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.6182 - acc: 0.0604\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.5943 - acc: 0.0598\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.5815 - acc: 0.0598\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.5881 - acc: 0.0598\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.5569 - acc: 0.0597\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.5367 - acc: 0.0598\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 151.5311 - acc: 0.0590\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.5147 - acc: 0.0594\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.4949 - acc: 0.0597\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.4813 - acc: 0.0598\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.4722 - acc: 0.0603\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.4524 - acc: 0.0602\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.4378 - acc: 0.0601\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.4293 - acc: 0.0593\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.4113 - acc: 0.0601\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.4066 - acc: 0.0596\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.3906 - acc: 0.0592\n",
      "training set(after 2450 iterations) : 0.926\n",
      "50\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.3859 - acc: 0.0594\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.3555 - acc: 0.0593\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.3413 - acc: 0.0593\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.3227 - acc: 0.0598\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.3135 - acc: 0.0595\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.2997 - acc: 0.0598\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.2831 - acc: 0.0597\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.2687 - acc: 0.0602\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.2552 - acc: 0.0589\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.2442 - acc: 0.0589\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.2338 - acc: 0.0595\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 151.2350 - acc: 0.0599\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 151.2149 - acc: 0.0594\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 151.1865 - acc: 0.0597\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 151.1667 - acc: 0.0593\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 151.1496 - acc: 0.0597\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.1438 - acc: 0.0597\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.1311 - acc: 0.0593\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.1074 - acc: 0.0590\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.0950 - acc: 0.0592\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 6us/step - loss: 151.0848 - acc: 0.0588\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 151.0674 - acc: 0.0589\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.0559 - acc: 0.0594\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.0395 - acc: 0.0589\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 151.0248 - acc: 0.0592\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 151.0123 - acc: 0.0587\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.9946 - acc: 0.0594\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.9862 - acc: 0.0592\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.9742 - acc: 0.0587\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.9542 - acc: 0.0588\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.9386 - acc: 0.0594\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.9258 - acc: 0.0594\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.9175 - acc: 0.0597\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.8957 - acc: 0.0595\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.8793 - acc: 0.0594\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.8657 - acc: 0.0586\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.8601 - acc: 0.0594\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.8394 - acc: 0.0586\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.8271 - acc: 0.0587\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.8134 - acc: 0.0587\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.8045 - acc: 0.0593\n",
      "Epoch 42/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.7892 - acc: 0.0588\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.7761 - acc: 0.0588\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.7535 - acc: 0.0588\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.7447 - acc: 0.0594\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.7249 - acc: 0.0588\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.7092 - acc: 0.0589\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.6982 - acc: 0.0589\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.6820 - acc: 0.0590\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.6683 - acc: 0.0589\n",
      "training set(after 2500 iterations) : 0.926\n",
      "51\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.6571 - acc: 0.0586\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.6448 - acc: 0.0587\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.6253 - acc: 0.0587\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.6150 - acc: 0.0584\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.5986 - acc: 0.0588\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.5840 - acc: 0.0590\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.5945 - acc: 0.0589\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.5622 - acc: 0.0589\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.5450 - acc: 0.0594\n",
      "Epoch 10/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.5390 - acc: 0.0595\n",
      "Epoch 11/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.5127 - acc: 0.0590\n",
      "Epoch 12/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.4980 - acc: 0.0593\n",
      "Epoch 13/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.4913 - acc: 0.0593\n",
      "Epoch 14/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.4711 - acc: 0.0587\n",
      "Epoch 15/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.4524 - acc: 0.0585\n",
      "Epoch 16/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.4467 - acc: 0.0589\n",
      "Epoch 17/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.4349 - acc: 0.0589\n",
      "Epoch 18/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.4173 - acc: 0.0590\n",
      "Epoch 19/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.3953 - acc: 0.0592\n",
      "Epoch 20/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.3938 - acc: 0.0593\n",
      "Epoch 21/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 150.3726 - acc: 0.0592\n",
      "Epoch 22/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.3529 - acc: 0.0588\n",
      "Epoch 23/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.3440 - acc: 0.0592\n",
      "Epoch 24/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.3342 - acc: 0.0595\n",
      "Epoch 25/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.3231 - acc: 0.0588\n",
      "Epoch 26/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.2992 - acc: 0.0586\n",
      "Epoch 27/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.2844 - acc: 0.0581\n",
      "Epoch 28/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.2756 - acc: 0.0586\n",
      "Epoch 29/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.2608 - acc: 0.0586\n",
      "Epoch 30/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.2461 - acc: 0.0593\n",
      "Epoch 31/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.2252 - acc: 0.0589\n",
      "Epoch 32/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.2113 - acc: 0.0586\n",
      "Epoch 33/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.2048 - acc: 0.0594\n",
      "Epoch 34/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.1873 - acc: 0.0592\n",
      "Epoch 35/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.1885 - acc: 0.0596\n",
      "Epoch 36/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.1779 - acc: 0.0589\n",
      "Epoch 37/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 150.1484 - acc: 0.0592\n",
      "Epoch 38/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.1491 - acc: 0.0593\n",
      "Epoch 39/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.1217 - acc: 0.0593\n",
      "Epoch 40/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.0973 - acc: 0.0590\n",
      "Epoch 41/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.0896 - acc: 0.0596\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.0863 - acc: 0.0595\n",
      "Epoch 43/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.0566 - acc: 0.0587\n",
      "Epoch 44/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.0428 - acc: 0.0589\n",
      "Epoch 45/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.0299 - acc: 0.0588\n",
      "Epoch 46/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 150.0156 - acc: 0.0587\n",
      "Epoch 47/50\n",
      "8908/8908 [==============================] - 0s 5us/step - loss: 150.0007 - acc: 0.0589\n",
      "Epoch 48/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 149.9870 - acc: 0.0588\n",
      "Epoch 49/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 149.9722 - acc: 0.0592\n",
      "Epoch 50/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 149.9542 - acc: 0.0593\n",
      "training set(after 2550 iterations) : 0.927\n",
      "52\n",
      "Epoch 1/50\n",
      "8908/8908 [==============================] - 0s 2us/step - loss: 149.9406 - acc: 0.0594\n",
      "Epoch 2/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 149.9257 - acc: 0.0593\n",
      "Epoch 3/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 149.9129 - acc: 0.0589\n",
      "Epoch 4/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 149.8999 - acc: 0.0592\n",
      "Epoch 5/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 149.8940 - acc: 0.0590\n",
      "Epoch 6/50\n",
      "8908/8908 [==============================] - 0s 4us/step - loss: 149.8691 - acc: 0.0588\n",
      "Epoch 7/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 149.8609 - acc: 0.0592\n",
      "Epoch 8/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 149.8466 - acc: 0.0596\n",
      "Epoch 9/50\n",
      "8908/8908 [==============================] - 0s 3us/step - loss: 149.8306 - acc: 0.0596\n",
      "Epoch 10/50\n",
      "5000/8908 [===============>..............] - ETA: 0s - loss: 150.7973 - acc: 0.0606"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d0a4b8e48b2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnormalized_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training set(after\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iterations) :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range (5000):\n",
    "    print(i)\n",
    "    normalized_model.fit(X, Y, epochs=50,verbose=1, batch_size=5000)\n",
    "    print(\"training set(after\", i*50+50, \"iterations) :\",str(pearsonr(normalized_model.predict(X).squeeze(),Y)[0])[0:5])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalized_dense_layer.input_shape)\n",
    "print(normalized_dense_layer.output_shape)\n",
    "print(normalized_dense_layer_2.input_shape)\n",
    "print(normalized_dense_layer_2.output_shape)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "normalized_model.summary()\n",
    "weights=normalized_model.get_weights()\n",
    "print(abs(weights[0]).mean())\n",
    "print(abs(weights[1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set(after 2650 iterations) : 0.895\n"
     ]
    }
   ],
   "source": [
    "print(\"test set(after\", i*50+50, \"iterations) :\",str(pearsonr(normalized_model.predict(Xtest).squeeze(),Ytest)[0])[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_model.save(\"./models/relu-\"+str(num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.models\n",
    "normalized_model=keras.models.load_model(\"./models/relu-\"+str(num_topics))\n",
    "normalized_model.summary()\n",
    "print(normalized_model.evaluate(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predict and evaluate pearson r\n",
    "normalized_prediction=normalized_model.predict(X).squeeze()\n",
    "print(str(len(normalized_prediction))+\" examples\")\n",
    "print(\"predicted length\")\n",
    "print(normalized_prediction[0:20])\n",
    "print(\"actual length\")\n",
    "print(Y[0:20])\n",
    "from scipy.stats.stats import pearsonr\n",
    "print(pearsonr(normalized_prediction,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#randomly sample to see the difference\n",
    "normalized_prediction=normalized_model.predict(X).squeeze()\n",
    "randX=abs(np.random.randn(len(Y),num_topics))\n",
    "randX=(randX.T/randX.sum(axis=1)).T\n",
    "randpred=normalized_model.predict(randX).squeeze()\n",
    "print(\"random prediction mean: \"+str(randpred.mean()))\n",
    "print(\"random prediction std: \"+str(np.std(randpred)))\n",
    "print(\"pearson of random: \"+str(pearsonr(randpred,Y)))\n",
    "print(\"pearson of actual prediction: \"+str(pearsonr(normalized_prediction,Y)))\n",
    "print(\"Y mean: \"+str(Y.mean()))\n",
    "pred=normalized_model.predict(X)\n",
    "print(\"Actual Prediction mean: \"+str(pred.mean()))\n",
    "print(\"Y std: \"+str(np.std(Y)))\n",
    "print(\"Actual Prediction std: \"+str(np.std(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you really think picking a heart surgeon (where you can check proficiency [and that proficiency persists]) is in any way similar to picking a money manager? predicted length: 12.007833 real length: 14\n",
      "\n",
      "All the guards on duty when Sandra Bland died were black or Latina. What possible motive would they have for murdering Sandra Bland? Why would they do such a thing knowing they would be the only possible suspects? . She was alone in the cell. Only the guards had access to her cell. There were no other inmates the guards could have blamed the murder on. Sandra Bland was a large woman. She was much larger than the Latina guards. The black male guard may have been able to overpower her, but there would have been a struggle. The autopsy showed no signs of a struggle. predicted length: 58.94391 real length: 46\n",
      "\n",
      "II still think DSK's behaviour vis a vis the hotel maid in New York City was reprehensible, but this case is not so clear, and I think the French got it right. We in the US would do well to adopt the French attitude on these matters.<br/><br/>Cheating on your spouse -- C'est la vie.<br/>Cheating on your taxes -- Sacrebleu! Explain yourself, Monsieur! predicted length: 27.217382 real length: 33\n",
      "\n",
      "If one looks at the last sentence in this article, it's possible to see what the problem is: the crazed, political, right-wingified refusal to sign off on the Law of the Sea makes it impossible to adjudicate this rational<br/><br/>These are the same lunatics who, after years of bargaining, refused to sign off on the international agreemrent regarding the disabled that we ourselves largely wrote, even with Bob Dole (wounded vet) there in a wheelchair. <br/><br/>There's little more than to say that this is demented. predicted length: 19.186953 real length: 41\n",
      "\n",
      "Not mentioned here, and it does matter, is that Mr. Garner had no \"illegal\" cigarettes on him. At the time of his murder he was doing nothing \"illegal\". predicted length: 11.135708 real length: 9\n",
      "\n",
      "As it should!    Of course there are those who think, no, know they would have played it differently. And life goes on to more important accomplishment. predicted length: 7.895138 real length: 10\n",
      "\n",
      "There is a black on black culture of violence depicting the low value they place on life.  Rap lyrics are their conscience. predicted length: 15.4512205 real length: 12\n",
      "\n",
      "From the photos I see in the news, it looks like many more young men than women in the fleeing masses. What will happen if the women are left behind., In India, there are more men than women and rape follows.<br/>My heart goes out to Europe. All of this mass migration will, no doubt, scare off<br/> tourists who help the European economy. predicted length: 18.564175 real length: 31\n",
      "\n",
      "Hey, guys. I'm a korean student who failed to get an admission from my dream college. Lately, I do feel very often that I do not want to live anymore.<br/>All my friends got accepted to wherever they wanted to go except for me. That fact kills me. I attended an elite secondary school in Korea, where every year, more than 30 students out of about 120 get accepted to IVY.(We had three harvard accepted students last year, the highest rate among the non-american high schools)<br/>I changed my mind in my senior year to go to a domestic college due to many factors such as financial problem.. but many of you guys have already known that particularly in korea, we have very competitive educational environment. I couldn't be matched by those who were preparing for their whole high school years to get admission from the domestic colleges. Now, I have to take KSAT(korean version of SAT) for the first time and I feel very depressed. Because If i fail in this year again, and not to go to the college im looking for, I will be labeled as super loser... hahaha ... maybe then, I wont exist on the earth anymore... :( Wish me luck on my upcoming test...  predicted length: 64.162674 real length: 103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for i in range(1,10):\n",
    "    print(data[i],'predicted length:', normalized_model.predict(Xtest)[i][0], 'real length:', Ytest[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Set Comment</th>\n",
       "      <th>Predicted Length</th>\n",
       "      <th>Actual Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To add to gfseiler'a comment, R.E. Lee was als...</td>\n",
       "      <td>83.923767</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you really think picking a heart surgeon (w...</td>\n",
       "      <td>12.007833</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All the guards on duty when Sandra Bland died ...</td>\n",
       "      <td>58.943909</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>II still think DSK's behaviour vis a vis the h...</td>\n",
       "      <td>27.217382</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If one looks at the last sentence in this arti...</td>\n",
       "      <td>19.186953</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not mentioned here, and it does matter, is tha...</td>\n",
       "      <td>11.135708</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>As it should!    Of course there are those who...</td>\n",
       "      <td>7.895138</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>There is a black on black culture of violence ...</td>\n",
       "      <td>15.451221</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From the photos I see in the news, it looks li...</td>\n",
       "      <td>18.564175</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Test Set Comment  Predicted Length  \\\n",
       "0  To add to gfseiler'a comment, R.E. Lee was als...         83.923767   \n",
       "1  Do you really think picking a heart surgeon (w...         12.007833   \n",
       "2  All the guards on duty when Sandra Bland died ...         58.943909   \n",
       "3  II still think DSK's behaviour vis a vis the h...         27.217382   \n",
       "4  If one looks at the last sentence in this arti...         19.186953   \n",
       "5  Not mentioned here, and it does matter, is tha...         11.135708   \n",
       "6  As it should!    Of course there are those who...          7.895138   \n",
       "7  There is a black on black culture of violence ...         15.451221   \n",
       "8  From the photos I see in the news, it looks li...         18.564175   \n",
       "\n",
       "   Actual Length  \n",
       "0             90  \n",
       "1             14  \n",
       "2             46  \n",
       "3             33  \n",
       "4             41  \n",
       "5              9  \n",
       "6             10  \n",
       "7             12  \n",
       "8             31  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df=pandas.DataFrame({\"Test Set Comment\":data[0:9],\"Predicted Length\":normalized_model.predict(Xtest)[0:9].squeeze(),\"Actual Length\":Ytest[0:9].squeeze()})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a black on black culture of violence depicting the low value they place on life.  Rap lyrics are their conscience.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 34</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 17</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>black</td>\n",
       "      <td>religious</td>\n",
       "      <td>love</td>\n",
       "      <td>gun</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>religion</td>\n",
       "      <td>great</td>\n",
       "      <td>people</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>race</td>\n",
       "      <td>freedom</td>\n",
       "      <td>hope</td>\n",
       "      <td>control</td>\n",
       "      <td>increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american</td>\n",
       "      <td>christian</td>\n",
       "      <td>feel</td>\n",
       "      <td>kill</td>\n",
       "      <td>rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>group</td>\n",
       "      <td>church</td>\n",
       "      <td>life</td>\n",
       "      <td>fear</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic 34    Topic 5 Topic 17  Topic 2  Topic 11\n",
       "0     black  religious     love      gun      high\n",
       "1     white   religion    great   people    number\n",
       "2      race    freedom     hope  control  increase\n",
       "3  american  christian     feel     kill      rate\n",
       "4     group     church     life     fear       low"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examplecomment=data[7]\n",
    "distro=Xtest[7]\n",
    "senttopics=np.argsort(distro)[-5:]\n",
    "senttopics=np.flip(senttopics)\n",
    "print(examplecomment)\n",
    "df={}\n",
    "for topicidx in senttopics[0:5]:\n",
    "    topicwords=[]\n",
    "    for topicword in topics[topicidx][0:5]:\n",
    "        topicwords.append(topicword[0])\n",
    "    df['Topic '+str(topicidx)]=topicwords\n",
    "df=pandas.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Lengths per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['foreign policy', 'argumentation on opinions', 'gun control', 'women’s rights', 'validity and truth claims', 'religion', 'links used', 'higher education', 'tax policy', 'public health issues', 'gender and transgender conversations', 'population and inflation', '[casual language]', '2016 primary elections (candidate emphasis)', 'judgments', 'global economic policy', 'journalism', 'positive artwork/music', 'healthcare', '[conversation priming]', 'familial discussions', '[n/a]', 'class and economics', 'police and criminal justice', 'environmental issues', 'cooking and gastronomy', '2016 primary elections (party emphasis)', 'legal discussions', 'climate change', 'greece bailout (article)', '[links]', 'television culture', 'transportation', '2016 primary elections (republican nominee emphasis)', 'race in america', 'n/a', 'cybersecurity', 'spending money (likely in reference to politics)', 'n/a?', 'military conflict (middle east emphasis)', 'work hours', 'conversation priming', 'nikki haley, south carolina governor, calls for removal of confederate battle flag (article)', 'soccer discussion', 'n/a?', 'jon stewart, patron saint of liberal smugness (article)', '“in zimbabwe, we don’t cry for lions” (article)', 'inside amazon: wrestling big ideas in a bruising workplace (article)', 'global immigration', 'new york housing discussion']\n"
     ]
    }
   ],
   "source": [
    "f=open(\"./plots/neuralnetwork-topics-50-labeled.txt\",\"r\")\n",
    "featuretext=f.read().lower()\n",
    "lines=featuretext.split(\"\\n\")\n",
    "feature_names=[]\n",
    "for line in lines:\n",
    "    if 'topic' in line:\n",
    "        feature_names.append(' '.join(line.split()[1:]))\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.39032258 24.03333333 36.0875     41.71038251 19.72173913 40.02427184\n",
      " 52.22916667 52.44394619 39.93258427 40.91121495 36.18181818 38.62745098\n",
      " 17.08695652 33.11471322 17.29761905 39.19248826 24.96       24.93532338\n",
      " 38.7755102  33.48863636 42.88383838 27.15584416 39.68067227 37.73702422\n",
      " 47.52       27.24904215 36.62934363 39.26222222 51.6015625  44.81446541\n",
      " 35.45652174 28.20481928 35.95530726 26.99122807 42.01442308 36.25301205\n",
      " 39.12077295 30.2295082  22.70833333 47.18604651 36.99090909 27.65467626\n",
      " 40.50793651 35.9        25.03333333 35.70512821 37.90419162 43.30252101\n",
      " 40.25396825 35.28      ]\n",
      "[ 7  6 28 24 39]\n",
      "['higher education' 'links used' 'climate change' 'environmental issues'\n",
      " 'military conflict (middle east emphasis)']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "averagelengths=np.zeros((50))\n",
    "numcomments=np.zeros(50)\n",
    "for sampleidx in range(Xtest.shape[0]):\n",
    "    topicidx=np.argmax(Xtest[sampleidx])\n",
    "    averagelengths[topicidx]+=Ytest[sampleidx]\n",
    "    numcomments[topicidx]+=1\n",
    "averagelengths/=numcomments\n",
    "print(averagelengths)\n",
    "longest=np.argsort(averagelengths)[-5:]\n",
    "longest=np.flip(longest)\n",
    "print(longest)\n",
    "feature_names=np.array(feature_names)\n",
    "print(feature_names[longest])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.barh(feature_names,averagelengths)\n",
    "plt.yticks(fontsize=4)\n",
    "plt.ylabel('Dominant Topic')\n",
    "plt.xlabel('Average Lengths per Topic')\n",
    "plt.savefig('./plots/AverageLengths.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#topic distribution skewness\n",
    "maxperc = []#largest topic percentage\n",
    "for sampleidx in range(X.shape[0]):\n",
    "    maxperc.append(np.max(X[sampleidx])*100)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(maxperc, bins = 50)\n",
    "plt.ylabel('Number of comments out of '+str(X.shape[0]))\n",
    "plt.xlabel('Largest topic percentage of a comment')\n",
    "plt.savefig('./plots/maxpercentage.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
